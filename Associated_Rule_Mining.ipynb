{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deshanchathusanka/data-mining/blob/main/Associated_Rule_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH78x0wdQxPJ",
        "outputId": "50868e66-2d40-4fec-9661-182ed2c2869d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting line_profiler\n",
            "  Downloading line_profiler-3.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: line-profiler\n",
            "Successfully installed line-profiler-3.5.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Small\" \"/content/\"\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Large\" \"/content/\"\n",
        "!pip3 install line_profiler\n",
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b5u30WFOTpxp"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c6KxqY0GTQBh"
      },
      "outputs": [],
      "source": [
        "def read_from_file(file_name) :\n",
        "  csvfile = open(file = file_name, mode = 'r') # open file\n",
        "  print(type(csvfile))\n",
        "\n",
        "  csvreader = csv.reader(csvfile, skipinitialspace = True) # csv reader object\n",
        "  rows = [] # empty list\n",
        "  for row in csvreader:\n",
        "    rows.append(row)\n",
        "\n",
        "  csvfile.close() #' close file\n",
        "  return rows\n",
        "\n",
        "def write_to_file(file_name, data) :\n",
        "  file = open(file = file_name, mode = 'w') # file object\n",
        "\n",
        "  csvwriter = csv.writer(file) # csv reader object\n",
        "  for row in data:\n",
        "    csvwriter.writerow(row)\n",
        "\n",
        "  file.close() # close file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VesjogsS1upZ",
        "outputId": "528ddaad-2a2c-46fa-f307-6a6190b3d73e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '_io.TextIOWrapper'>\n"
          ]
        }
      ],
      "source": [
        "transactions = read_from_file('Dataset.Small/GroceryStore.csv') # read transactions or database\n",
        "write_to_file('Transactions.csv', transactions) # write transactions or database into another file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiyEGq5r1Xm"
      },
      "source": [
        "**Brute-Force Approach(Frequent itemset mining)**\n",
        "\n",
        "$\n",
        "Number\\ of\\ unique\\ items\\ =\\ d\\\\\n",
        "Number\\ of\\ transactions\\ =\\ N\\\\\n",
        "Average\\ width\\ of\\ a\\ transaction=\\ w\\\\ \\\\\n",
        "Number\\ of\\ all\\ possible\\ combinations\\ =\\\n",
        "\\displaystyle\\sum_{r=1} ^{d} c_{r}\\ =\\ 2^d - 1\\\\\n",
        "Complexity\\ =\\ O(Nw2^{d})\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nxQWILyHvSVr"
      },
      "outputs": [],
      "source": [
        "def search_database(database, itemset) :\n",
        "  \"\"\"\n",
        "    Search occurance of given itemset in the transaction database\n",
        "    Inputs\n",
        "      database : Transactions : List of lists \n",
        "      itemset : Itemset that should be searched : List\n",
        "    Outputs\n",
        "      occurance : Number of occurance : integer\n",
        "  \"\"\"\n",
        "  frequency = np.count_nonzero([all(item in transaction for item in itemset) for transaction in database])\n",
        "  return frequency\n",
        "\n",
        "def calc_candidate_sup_cnt(database, c_k, k) : \n",
        "  \"\"\"\n",
        "  Calculate support count for all candidate k-itemsets and generate dataframe\n",
        "  Inputs\n",
        "    database : Transactions : List of lists\n",
        "    c_k : Candidate k-itemsets : List of tuples\n",
        "  Output : \n",
        "    c_k_df : Candidate k-itemsets dataframe with support counts\n",
        "  \"\"\"\n",
        "  itemset_df = pd.DataFrame(columns=['key', 'item_cnt','itemset','support_cnt'])\n",
        "  for itemset in c_k: # itemset : tuple of items\n",
        "      support_count = search_database(database, itemset)\n",
        "      data_entry = {\n",
        "        \"key\" : frozenset(itemset), # immutable set\n",
        "        \"item_cnt\": k,\n",
        "        \"itemset\": itemset,\n",
        "        \"support_cnt\": support_count\n",
        "      }\n",
        "      itemset_df = itemset_df.append(data_entry, ignore_index = True)\n",
        "  return itemset_df\n",
        "\n",
        "def select_freq_itemsets(c_k, min_sup_cnt) :\n",
        "  \"\"\"\n",
        "  Select frequent k-itemsets from candidate k-itemsets(Candidate elimination)\n",
        "  Inputs\n",
        "    c_k : Candidate k-itemsets : DataFrame(itemset,support_cnt)\n",
        "    min_sup_cnt : minimum support count(hyper parameter) : integer\n",
        "  Outputs\n",
        "    f_k : Frequent k-itemsets : DataFrame(itemset,support_cnt)\n",
        "  \"\"\"\n",
        "  f_k = c_k[c_k['support_cnt'] >= min_sup_cnt] \n",
        "  return f_k\n",
        "\n",
        "def brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups):\n",
        "  \"\"\" \n",
        "  Brute Force Method \n",
        "  Inputs\n",
        "    database : Transaction database : List of lists\n",
        "    unique_items : Unique items in the database : List of items\n",
        "    min_sup_cnt : Minimum support count : intereger\n",
        "    itemset_groups : Required item groups(k values)\n",
        "  Outputs\n",
        "    all_freq_itemsets = Frequent itemsets for given groups = DataFrame\n",
        "  \"\"\"\n",
        "  all_candidate_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups: # loop number of itemset groups\n",
        "    c_k = combinations(unique_items, k) # create combinations for each itemset group\n",
        "    c_k_df = calc_candidate_sup_cnt(database, c_k, k)\n",
        "    all_candidate_df = all_candidate_df.append(c_k_df, ignore_index = True)\n",
        "\n",
        "  all_freq_itemsets = select_freq_itemsets(all_candidate_df, min_sup_cnt)\n",
        "  return all_freq_itemsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcLQWK_7lej",
        "outputId": "3b12fcfe-67b2-4c4e-eed2-c09cecf84667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of transactions : 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (3,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of large dataset : (541910, 8)\n",
            "<bound method NDFrame.head of                 0          1                                    2         3  \\\n",
            "0       InvoiceNo  StockCode                          Description  Quantity   \n",
            "1          536365     85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
            "2          536365      71053                  WHITE METAL LANTERN         6   \n",
            "3          536365     84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
            "4          536365     84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
            "...           ...        ...                                  ...       ...   \n",
            "541905     581587      22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
            "541906     581587      22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
            "541907     581587      23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
            "541908     581587      23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
            "541909     581587      22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
            "\n",
            "                       4          5           6               7  \n",
            "0            InvoiceDate  UnitPrice  CustomerID         Country  \n",
            "1       01/12/2010 08:26       2.55       17850  United Kingdom  \n",
            "2       01/12/2010 08:26       3.39       17850  United Kingdom  \n",
            "3       01/12/2010 08:26       2.75       17850  United Kingdom  \n",
            "4       01/12/2010 08:26       3.39       17850  United Kingdom  \n",
            "...                  ...        ...         ...             ...  \n",
            "541905  09/12/2011 12:50       0.85     12680.0          France  \n",
            "541906  09/12/2011 12:50        2.1     12680.0          France  \n",
            "541907  09/12/2011 12:50       4.15     12680.0          France  \n",
            "541908  09/12/2011 12:50       4.15     12680.0          France  \n",
            "541909  09/12/2011 12:50       4.95     12680.0          France  \n",
            "\n",
            "[541910 rows x 8 columns]>\n",
            "\n",
            "\n",
            "Shape of frequent itemsets : (35, 4)\n",
            "Frequent itemsets from brute-force approach \n",
            "  item_cnt       itemset support_cnt          key\n",
            "0        1       (MILK,)           5       (MILK)\n",
            "1        1      (MAGGI,)           5      (MAGGI)\n",
            "2        1  (BOURNVITA,)           3  (BOURNVITA)\n",
            "3        1     (COFFEE,)           6     (COFFEE)\n",
            "4        1    (BISCUIT,)           6    (BISCUIT)\n",
            "Frequent itemsets from brute-force approach \n",
            "    item_cnt                 itemset support_cnt                     key\n",
            "64         2       (TEA, CORNFLAKES)           2       (CORNFLAKES, TEA)\n",
            "90         3  (MILK, BISCUIT, BREAD)           2  (MILK, BISCUIT, BREAD)\n",
            "129        3   (MAGGI, BISCUIT, TEA)           2   (MAGGI, BISCUIT, TEA)\n",
            "134        3     (MAGGI, BREAD, TEA)           2     (TEA, MAGGI, BREAD)\n",
            "135        3     (MAGGI, BREAD, JAM)           2     (MAGGI, JAM, BREAD)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Small dataset\"\"\"\n",
        "transaction_df = pd.read_csv('Dataset.Small/GroceryStore.csv', header=None)\n",
        "transaction_df.drop_duplicates(subset =0,\n",
        "                     keep = 'first', inplace = True)\n",
        "database = list(transaction_df[0].apply(lambda x:x.split(\",\") ))\n",
        "print(f'Size of transactions : {len(database)}')\n",
        "\n",
        "unique_items = set()\n",
        "for transaction in database:\n",
        "  for item in transaction :\n",
        "    unique_items.add(item)\n",
        "# print(unique_items)\n",
        "\n",
        "\n",
        "\"\"\" Large dataset \"\"\"\n",
        "transaction_df_large = pd.read_csv('Dataset.Large/OnlineRetail.csv', header=None)\n",
        "print(f'Size of large dataset : {transaction_df_large.shape}')\n",
        "print(transaction_df_large.head)\n",
        "\n",
        "min_sup_cnt = 2\n",
        "itemset_groups = [1, 2, 3, 4, 5]\n",
        "freq_itemsets = brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups)\n",
        "print(f'\\n\\nShape of frequent itemsets : {freq_itemsets.shape}')\n",
        "print(f'Frequent itemsets from brute-force approach \\n{freq_itemsets.head()}')\n",
        "print(f'Frequent itemsets from brute-force approach \\n{freq_itemsets.tail()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUYANEetVox9"
      },
      "source": [
        "**Apriori Algorithm(Frequent itemset mining)**\n",
        "\n",
        "* Superset is frequent >>> Subset is frequent (Bottom Up)\n",
        "\n",
        "* Sebset is infrequent >>> Superset is infrequent (Top down)\n",
        "\n",
        "* This algorithm has **anti-monotone** propety >>> support of itemset can not exceed support of its subset\n",
        "\n",
        "\n",
        "**Main Steps**\n",
        "\n",
        "* Candidate Generation ($F_{k-1} >> C_{k}$)\n",
        "\n",
        "* Candidate Pruning\n",
        "\n",
        "* Support Counting\n",
        "\n",
        "* Candidate Elimination ($C_{k} >> F_{k}$)\n",
        "\n",
        "**Candidate Pruning ($F_{k-1}×F_{k-1}$)**\n",
        "\n",
        "$Itemset\\ is\\ not\\ frequent\\ if\\ one\\ of\\ its\\ sub\\ itemset\\ is\\ not\\ frequent$\n",
        "\n",
        "* Number of **(k-1)-size subsets** for a **k-itemset** = $C^{K}_{K-1}$ = $K$\n",
        "* Number of **already verified (k-1)-size subsets** at the candidate generation = $2$\n",
        "* Number of subsets for frequency verification stage per each **k-itemset** = $K-2$\n",
        "* Total number of subsets for frequency verification for **all k-itemsets** = $L_{k}\\times(K-2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ufd1lKSaVv6K"
      },
      "outputs": [],
      "source": [
        "def generate_candidates(f_prv, k):\n",
        "  \"\"\"\n",
        "  Generate candidate set for k-itemsets\n",
        "  Input : \n",
        "    f_prv : F(K-1) \n",
        "  Output : \n",
        "    c_k : List of candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  c_k = [] # candidate k-itemset\n",
        "  f_prv_pairs = combinations(f_prv, 2) # select pair of frequent (k-1)-itemsets\n",
        "  for f_prv_pair in f_prv_pairs:\n",
        "    if(len(f_prv_pair[0]) == 1) : ### K=2 ###\n",
        "      two_itemset = [*f_prv_pair[0],*f_prv_pair[1]]\n",
        "      two_itemset.sort()         \n",
        "      c_k.append(two_itemset)\n",
        "    elif(f_prv_pair[0][0:k-2] == f_prv_pair[1][0:k-2]) :### K>=2 and first (k-2) items are same in both itemsets ###\n",
        "      generated_itemset = f_prv_pair[0][0:k-2]\n",
        "      delta_items = [f_prv_pair[0][k-2],f_prv_pair[1][k-2]]\n",
        "      delta_items.sort()\n",
        "      generated_itemset = [*generated_itemset,*delta_items]\n",
        "      c_k.append(generated_itemset)\n",
        "  return c_k\n",
        "\n",
        "def prune_candidates(c_k, f_prv, k) :\n",
        "  \"\"\"\n",
        "  Prune candidate set from k-itemsets\n",
        "  Inputs\n",
        "    c_k : candidate k-itemsets : List of lists\n",
        "    f_prv : frequent (k-1)-itemsets \n",
        "  Output\n",
        "    c_k_prune : pruned candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  infrq_itemsets = []\n",
        "  ### search candidate k-itemsets to check frequency of its (k-1)-sized subsets\n",
        "  for itemset in c_k :\n",
        "    subsets = combinations(itemset, k-1)\n",
        "    for subset in subsets:\n",
        "      frequency = search_database(f_prv, subset)\n",
        "      if(frequency==0):\n",
        "        infrq_itemsets.append(itemset)\n",
        "        break\n",
        "  ### remove infrequent itemsets from candidate k-itemsets ###\n",
        "  for itemset in infrq_itemsets :\n",
        "    c_k.remove(itemset)\n",
        "\n",
        "  return c_k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnYQvEz__3Nn",
        "outputId": "428544b6-10c2-416f-9fe0-9cc4cae5f47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of frequent itemsets : (35, 4)\n",
            "Frequent itemsets from apriori approach \n",
            "  item_cnt      itemset support_cnt          key\n",
            "0        1       [MILK]           5       (MILK)\n",
            "1        1      [MAGGI]           5      (MAGGI)\n",
            "2        1  [BOURNVITA]           3  (BOURNVITA)\n",
            "3        1     [COFFEE]           6     (COFFEE)\n",
            "4        1    [BISCUIT]           6    (BISCUIT)\n",
            "Frequent itemsets from apriori approach \n",
            "   item_cnt                 itemset support_cnt                     key\n",
            "30        2       [CORNFLAKES, TEA]           2       (CORNFLAKES, TEA)\n",
            "31        3  [BISCUIT, BREAD, MILK]           2  (MILK, BISCUIT, BREAD)\n",
            "32        3   [BISCUIT, MAGGI, TEA]           2   (MAGGI, BISCUIT, TEA)\n",
            "33        3     [BREAD, MAGGI, TEA]           2     (TEA, MAGGI, BREAD)\n",
            "34        3     [BREAD, JAM, MAGGI]           2     (MAGGI, JAM, BREAD)\n",
            "             key item_cnt       itemset support_cnt\n",
            "0         (MILK)        1        [MILK]           5\n",
            "1        (MAGGI)        1       [MAGGI]           5\n",
            "2    (BOURNVITA)        1   [BOURNVITA]           3\n",
            "3       (COFFEE)        1      [COFFEE]           6\n",
            "4      (BISCUIT)        1     [BISCUIT]           6\n",
            "5        (BREAD)        1       [BREAD]          11\n",
            "6        (SUGER)        1       [SUGER]           5\n",
            "7         (COCK)        1        [COCK]           2\n",
            "8          (TEA)        1         [TEA]           6\n",
            "9          (JAM)        1         [JAM]           2\n",
            "10  (CORNFLAKES)        1  [CORNFLAKES]           5\n",
            "                      key item_cnt                itemset support_cnt\n",
            "3         (MILK, BISCUIT)        2        [BISCUIT, MILK]           2\n",
            "4           (MILK, BREAD)        2          [BREAD, MILK]           4\n",
            "9      (MILK, CORNFLAKES)        2     [CORNFLAKES, MILK]           2\n",
            "12       (MAGGI, BISCUIT)        2       [BISCUIT, MAGGI]           2\n",
            "13         (MAGGI, BREAD)        2         [BREAD, MAGGI]           3\n",
            "16           (MAGGI, TEA)        2           [MAGGI, TEA]           4\n",
            "17           (MAGGI, JAM)        2           [JAM, MAGGI]           2\n",
            "21     (BOURNVITA, BREAD)        2     [BOURNVITA, BREAD]           2\n",
            "22     (BOURNVITA, SUGER)        2     [BOURNVITA, SUGER]           2\n",
            "28        (COFFEE, BREAD)        2        [BREAD, COFFEE]           2\n",
            "29        (COFFEE, SUGER)        2        [COFFEE, SUGER]           3\n",
            "30         (COFFEE, COCK)        2         [COCK, COFFEE]           2\n",
            "33   (COFFEE, CORNFLAKES)        2   [COFFEE, CORNFLAKES]           3\n",
            "34       (BISCUIT, BREAD)        2       [BISCUIT, BREAD]           4\n",
            "37         (BISCUIT, TEA)        2         [BISCUIT, TEA]           2\n",
            "39  (BISCUIT, CORNFLAKES)        2  [BISCUIT, CORNFLAKES]           2\n",
            "40         (SUGER, BREAD)        2         [BREAD, SUGER]           3\n",
            "42           (TEA, BREAD)        2           [BREAD, TEA]           3\n",
            "43           (JAM, BREAD)        2           [BREAD, JAM]           2\n",
            "53      (CORNFLAKES, TEA)        2      [CORNFLAKES, TEA]           2\n",
            "                      key item_cnt                 itemset support_cnt\n",
            "0  (MILK, BISCUIT, BREAD)        3  [BISCUIT, BREAD, MILK]           2\n",
            "3   (MAGGI, BISCUIT, TEA)        3   [BISCUIT, MAGGI, TEA]           2\n",
            "4     (TEA, MAGGI, BREAD)        3     [BREAD, MAGGI, TEA]           2\n",
            "5     (MAGGI, JAM, BREAD)        3     [BREAD, JAM, MAGGI]           2\n"
          ]
        }
      ],
      "source": [
        "def apriori_algorithm(min_sup_cnt) :\n",
        "  f_itemsets_store = {}\n",
        "  all_frq_itemsets_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups:\n",
        "    if(k>1) :\n",
        "      f_prv = f_itemsets_store[k-1]['itemset']\n",
        "      c_k = generate_candidates(f_prv, k) # generate candidates\n",
        "      c_k = prune_candidates(c_k, f_prv, k) # prune candidates\n",
        "      c_k = calc_candidate_sup_cnt(database, c_k, k)\n",
        "      f_k = select_freq_itemsets(c_k, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_k, ignore_index=True)\n",
        "      f_itemsets_store[k] = f_k\n",
        "    else :\n",
        "      c_1 = [[item] for item in unique_items] # candidate 1-itemset\n",
        "      c_1 = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "      f_1 = select_freq_itemsets(c_1, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_1, ignore_index=True)\n",
        "      f_itemsets_store[1] = f_1\n",
        "  return all_frq_itemsets_df, f_itemsets_store\n",
        "\n",
        "# all_itemsets_df = %lprun -f apriori_algorithm apriori_algorithm() # execute line profiler\n",
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2) # execute line profiler\n",
        "print(f'Shape of frequent itemsets : {all_frq_itemsets_df.shape}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{all_frq_itemsets_df.head()}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{all_frq_itemsets_df.tail()}')\n",
        "\n",
        "print(f_itemsets_store[1])\n",
        "print(f_itemsets_store[2])\n",
        "print(f_itemsets_store[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDqu5cMCV8LD"
      },
      "source": [
        "**Rule Generation**\n",
        "\n",
        "Antecedent -> Consequent\n",
        "\n",
        "Ex : {X,Y} -> {P,Q}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JglQrjGLWedO"
      },
      "outputs": [],
      "source": [
        "def generate_rules(f_k, h_m, f_itemsets_store, min_conf) :\n",
        "  \"\"\"\n",
        "  Generate rules from frequent itemsets\n",
        "  Inputs:\n",
        "    f_k : Frequent k-itemsets (X,Y,P,Q) : Tuple\n",
        "    h_m : Consequent (P,Q) : Tuple\n",
        "    f_itemsets_store : Frequent itemset store(key = k, value = support counts) : Dictionary(Key = Integer, value = DataFrame)\n",
        "  \"\"\"\n",
        "  conf = 0\n",
        "  if(len(h_m) > 0) :\n",
        "    k = len(f_k)\n",
        "    m = len(h_m[0])\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "  \n",
        "  if(k > m+1) : #### Check whether an item can be passed from antecedent to consequent >>> Terminating condition of the recursive function###\n",
        "    h_next = generate_candidates(h_m, m+1) # Generate (m+1) candidates for consequent\n",
        "    h_next = prune_candidates(h_next, h_m, m+1) # Prune (m+1) candidates\n",
        "    h_next_invalid = []\n",
        "    rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "    for h_next_i in h_next : # Iterate candidate consequents\n",
        "      consequent = h_next_i\n",
        "      antecedent = list(set(f_k) - set(h_next_i))\n",
        "      rule_sup_cnt = f_itemsets_store[k].loc[f_itemsets_store[k]['key']==frozenset(f_k)]\n",
        "      antecedent_sup_cnt = f_itemsets_store[len(antecedent)].loc[f_itemsets_store[len(antecedent)]['key']==frozenset(antecedent)]\n",
        "\n",
        "      if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "        conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "      if(conf > min_conf) : \n",
        "        data_entry = {\n",
        "          'antecedent': antecedent,\n",
        "          'consequent': consequent,\n",
        "          'confidence': conf\n",
        "        }\n",
        "        rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "      else : \n",
        "        h_next_invalid.append(h_next_i)\n",
        "\n",
        "    for h_next_invalid_i in h_next_invalid : # Remove invalid consequents \n",
        "      h_next.remove(h_next_invalid_i)\n",
        "\n",
        "    ### recursively generate rules for subgraphs \n",
        "    return generate_rules(f_k, h_next, f_itemsets_store, min_conf).append(rule_df, ignore_index=True)\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l5XsaZHP9m0",
        "outputId": "8206d8e6-12bd-4a72-bf82-b9c31d9542d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(35, 4)\n",
            "          antecedent      consequent  confidence\n",
            "0             [MILK]         [BREAD]    0.800000\n",
            "1            [MAGGI]         [BREAD]    0.600000\n",
            "2              [TEA]         [MAGGI]    0.666667\n",
            "3            [MAGGI]           [TEA]    0.800000\n",
            "4              [JAM]         [MAGGI]    1.000000\n",
            "5        [BOURNVITA]         [BREAD]    0.666667\n",
            "6        [BOURNVITA]         [SUGER]    0.666667\n",
            "7            [SUGER]        [COFFEE]    0.600000\n",
            "8           [COFFEE]         [SUGER]    0.500000\n",
            "9             [COCK]        [COFFEE]    1.000000\n",
            "10      [CORNFLAKES]        [COFFEE]    0.600000\n",
            "11          [COFFEE]    [CORNFLAKES]    0.500000\n",
            "12         [BISCUIT]         [BREAD]    0.666667\n",
            "13           [SUGER]         [BREAD]    0.600000\n",
            "14             [TEA]         [BREAD]    0.500000\n",
            "15             [JAM]         [BREAD]    1.000000\n",
            "16     [MILK, BREAD]       [BISCUIT]    0.500000\n",
            "17   [MILK, BISCUIT]         [BREAD]    1.000000\n",
            "18  [BISCUIT, BREAD]          [MILK]    0.500000\n",
            "19      [MAGGI, TEA]       [BISCUIT]    0.500000\n",
            "20    [BISCUIT, TEA]         [MAGGI]    1.000000\n",
            "21  [MAGGI, BISCUIT]           [TEA]    1.000000\n",
            "22      [MAGGI, TEA]         [BREAD]    0.500000\n",
            "23      [BREAD, TEA]         [MAGGI]    0.666667\n",
            "24    [MAGGI, BREAD]           [TEA]    0.666667\n",
            "25      [MAGGI, JAM]         [BREAD]    1.000000\n",
            "26    [MAGGI, BREAD]           [JAM]    0.666667\n",
            "27      [JAM, BREAD]         [MAGGI]    1.000000\n",
            "28             [JAM]  [BREAD, MAGGI]    1.000000\n",
            "\n",
            "\n",
            "Shape of association rules : (29, 3)\n",
            "Association rules(Head) \n",
            "  antecedent consequent  confidence\n",
            "0     [MILK]    [BREAD]    0.800000\n",
            "1    [MAGGI]    [BREAD]    0.600000\n",
            "2      [TEA]    [MAGGI]    0.666667\n",
            "3    [MAGGI]      [TEA]    0.800000\n",
            "4      [JAM]    [MAGGI]    1.000000\n",
            "Association rules(Tail) \n",
            "        antecedent      consequent  confidence\n",
            "24  [MAGGI, BREAD]           [TEA]    0.666667\n",
            "25    [MAGGI, JAM]         [BREAD]    1.000000\n",
            "26  [MAGGI, BREAD]           [JAM]    0.666667\n",
            "27    [JAM, BREAD]         [MAGGI]    1.000000\n",
            "28           [JAM]  [BREAD, MAGGI]    1.000000\n"
          ]
        }
      ],
      "source": [
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2)\n",
        "min_conf = 0.5\n",
        "print(all_frq_itemsets_df.shape)\n",
        "\n",
        "rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "for f_k in all_frq_itemsets_df['itemset'] :\n",
        "  h_1_invalid = []\n",
        "  for item in f_k :\n",
        "    if len(f_k)==1 :\n",
        "      continue\n",
        "    consequent = []\n",
        "    consequent.append(item)\n",
        "    antecedent = list(set(f_k)-set(consequent))\n",
        "\n",
        "    rule_sup_cnt = f_itemsets_store[len(f_k)].loc[f_itemsets_store[len(f_k)]['key']==frozenset(f_k)]\n",
        "    antecedent_sup_cnt = f_itemsets_store[len(antecedent)].loc[f_itemsets_store[len(antecedent)]['key']==frozenset(antecedent)]\n",
        "\n",
        "    if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "      conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "    if(conf >= min_conf) : \n",
        "      data_entry = {\n",
        "        'antecedent': antecedent,\n",
        "        'consequent': consequent,\n",
        "        'confidence': conf\n",
        "      }\n",
        "      rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "    else : \n",
        "      h_1_invalid.append(consequent)\n",
        "\n",
        "  h_1 = [[item] for item in f_k] # create consequents for single items\n",
        "  if len(h_1_invalid)>0 : # remove low confidence items from consequents\n",
        "    for h_1_invalid_i in h_1_invalid : # Remove invalid consequents \n",
        "      h_1.remove(h_1_invalid_i)\n",
        "  if len(h_1)==0 :\n",
        "    continue\n",
        "    \n",
        "  rule_df = rule_df.append(generate_rules(f_k, h_1, f_itemsets_store, min_conf), ignore_index=True)\n",
        "\n",
        "print(rule_df)\n",
        "print(f'\\n\\nShape of association rules : {rule_df.shape}')\n",
        "print(f'Association rules(Head) \\n{rule_df.head()}')\n",
        "print(f'Association rules(Tail) \\n{rule_df.tail()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3kK-iwgTmIH"
      },
      "source": [
        "**FP-Growth Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dVp5lYHcX7sS"
      },
      "outputs": [],
      "source": [
        "class ItemData :\n",
        "  def __init__(self, item, sup_count):\n",
        "    self.item = item\n",
        "    self.sup_count = sup_count\n",
        "    self.con_sup_cnt = 0\n",
        "\n",
        "  def __str__(self):\n",
        "     return str({\n",
        "         \"item\" : self.item,\n",
        "         \"sup_count\" : self.sup_count\n",
        "     })\n",
        "\n",
        "  def __cmp__(self,other):\n",
        "    return self.count<other.count\n",
        "\n",
        "class TreeNode :\n",
        "  def __init__(self, data):\n",
        "      self.parent = None\n",
        "      self.children = {}\n",
        "      self.next = None\n",
        "      self.reference = None\n",
        "      self.data = data\n",
        "\n",
        "class ReferenceNode : \n",
        "  def __init__(self) :\n",
        "    self.head = None\n",
        "    self.tail = None\n",
        "    self.sup_count = 0\n",
        "    self.con_sup_cnt = 0\n",
        "    self.item = None\n",
        "  \n",
        "  def __str__(self):\n",
        "    return str({\n",
        "        \"item\" : self.item,\n",
        "        \"sup_count\" : self.sup_count,\n",
        "        \"con_sup_count\" : self.con_sup_cnt\n",
        "    })\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(self.item)\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return (\n",
        "    self.__class__ == other.__class__ and\n",
        "            self.item == other.item )\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ6oD_scdMKv",
        "outputId": "c10e066a-0561-4f35-b3e6-29ad86f3178b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infrequent Row : Empty DataFrame\n",
            "Columns: [key, item_cnt, itemset, support_cnt]\n",
            "Index: []\n",
            "\n",
            "Infrequent Items : set()\n",
            "\n",
            "Frequent single items :              key item_cnt       itemset support_cnt\n",
            "5        (BREAD)        1       [BREAD]          11\n",
            "3       (COFFEE)        1      [COFFEE]           6\n",
            "4      (BISCUIT)        1     [BISCUIT]           6\n",
            "8          (TEA)        1         [TEA]           6\n",
            "0         (MILK)        1        [MILK]           5\n",
            "1        (MAGGI)        1       [MAGGI]           5\n",
            "6        (SUGER)        1       [SUGER]           5\n",
            "10  (CORNFLAKES)        1  [CORNFLAKES]           5\n",
            "2    (BOURNVITA)        1   [BOURNVITA]           3\n",
            "7         (COCK)        1        [COCK]           2\n",
            "9          (JAM)        1         [JAM]           2\n",
            "\n",
            "\n",
            "Support Count of BREAD : 11\n",
            "{'item': 'BREAD', 'sup_count': 11}\n",
            "\n",
            "Support Count of BISCUIT : 6\n",
            "{'item': 'BISCUIT', 'sup_count': 3}\n",
            "{'item': 'BISCUIT', 'sup_count': 2}\n",
            "{'item': 'BISCUIT', 'sup_count': 1}\n",
            "\n",
            "Support Count of MILK : 5\n",
            "{'item': 'MILK', 'sup_count': 2}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "\n",
            "Support Count of CORNFLAKES : 5\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "\n",
            "Support Count of TEA : 6\n",
            "{'item': 'TEA', 'sup_count': 3}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "\n",
            "Support Count of BOURNVITA : 3\n",
            "{'item': 'BOURNVITA', 'sup_count': 1}\n",
            "{'item': 'BOURNVITA', 'sup_count': 1}\n",
            "{'item': 'BOURNVITA', 'sup_count': 1}\n",
            "\n",
            "Support Count of MAGGI : 5\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "\n",
            "Support Count of JAM : 2\n",
            "{'item': 'JAM', 'sup_count': 1}\n",
            "{'item': 'JAM', 'sup_count': 1}\n",
            "\n",
            "Support Count of COFFEE : 6\n",
            "{'item': 'COFFEE', 'sup_count': 1}\n",
            "{'item': 'COFFEE', 'sup_count': 3}\n",
            "{'item': 'COFFEE', 'sup_count': 2}\n",
            "\n",
            "Support Count of COCK : 2\n",
            "{'item': 'COCK', 'sup_count': 1}\n",
            "{'item': 'COCK', 'sup_count': 1}\n",
            "\n",
            "Support Count of SUGER : 5\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "\n",
            "I : ['JAM']\n",
            "I : ['MAGGI', 'JAM']\n",
            "I : ['TEA', 'MAGGI', 'JAM']\n"
          ]
        }
      ],
      "source": [
        "def generate_fp_tree(min_sup_cnt):\n",
        "\n",
        "  ############################# Part (1) ##################################################\n",
        "  c_1 = [[item] for item in unique_items]\n",
        "  itemset_df = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "\n",
        "  infrequent_rows = itemset_df[ itemset_df['support_cnt'] < min_sup_cnt ] # filter items with lower confidence and retrieve indices of them\n",
        "  print(f'Infrequent Row : {infrequent_rows}\\n')\n",
        "  infrequent_items = set(row['itemset'][0] for index,row in infrequent_rows.iterrows()) ################################## TODO : Need to review ##########################################\n",
        "  print( f'Infrequent Items : {infrequent_items}\\n')\n",
        "  \n",
        "  #### Drop infrequent items ####\n",
        "  itemset_df.drop(infrequent_rows.index , inplace=True)\n",
        "  #### Sort according to support count ####\n",
        "  itemset_df = itemset_df.sort_values(by = ['support_cnt'], ascending=False)\n",
        "  print(f'Frequent single items : {itemset_df}\\n\\n')\n",
        " \n",
        "  #### Remove infrequent items from every transaction and sort by support count in descending order ####\n",
        "  database_node_model = []\n",
        "  for transaction in database:\n",
        "    transaction_prune = set(transaction)-infrequent_items # remove infrequent items from every transaction\n",
        "\n",
        "    transaction_node_model = []\n",
        "    for item in transaction_prune : \n",
        "      sup_cnt = itemset_df[itemset_df['key']==frozenset([item])]['support_cnt'].values[0]\n",
        "      #### Create item object for each item in every transaction ###\n",
        "      item_data = ItemData(item, sup_cnt)\n",
        "      transaction_node_model.append(item_data)\n",
        "    \n",
        "    #### Sort item nodes in transaction accoring to support count (Descending order) ####\n",
        "    transaction_node_model.sort(key = lambda c: c.sup_count, reverse=True) \n",
        "    database_node_model.append(transaction_node_model)\n",
        "\n",
        "  ############################# Part (2) ##################################################\n",
        "  \n",
        "  #### Root Node : Null ####\n",
        "  root_data = ItemData(item = None, sup_count = None)\n",
        "  root_node = TreeNode(root_data) \n",
        "  #### Item chain maintain dictionary ####\n",
        "  reference_keeper = {}\n",
        "  \n",
        "  ### Construct the model ####\n",
        "  for transaction in database_node_model :\n",
        "    current_root_locator = root_node\n",
        "    for search_data in transaction:\n",
        "      current_root_locator = traverse(current_root_locator, search_data, reference_keeper)\n",
        "\n",
        "  return root_node, reference_keeper\n",
        "\n",
        "\n",
        "\n",
        "def traverse(root_node, search_data, reference_keeper):\n",
        "  current_root_locator = root_node\n",
        "  if search_data.item not in current_root_locator.children:\n",
        "    search_data.sup_count = 1\n",
        "    search_node = TreeNode(search_data)\n",
        "    search_node.parent = current_root_locator\n",
        "    current_root_locator.children[search_data.item] = search_node\n",
        "    current_root_locator = search_node\n",
        "\n",
        "    if search_data.item not in reference_keeper :\n",
        "      ##### Create reference for the new item ####\n",
        "      reference_node = ReferenceNode()\n",
        "      reference_node.head = search_node\n",
        "      reference_node.tail = search_node\n",
        "      reference_node.sup_count = search_data.sup_count\n",
        "      reference_node.item = search_data.item\n",
        "      reference_keeper[search_data.item] = reference_node\n",
        "      search_node.reference = reference_node\n",
        "    else :\n",
        "      reference_node = reference_keeper[search_data.item]\n",
        "      tail_node = reference_node.tail\n",
        "      tail_node.next = search_node\n",
        "      reference_node.tail = search_node\n",
        "      reference_node.sup_count += search_data.sup_count\n",
        "      reference_keeper[search_data.item] = reference_node\n",
        "      search_node.reference = reference_node\n",
        "  else :\n",
        "    current_root_locator = current_root_locator.children[search_data.item]\n",
        "    current_root_locator.data.sup_count += 1\n",
        "\n",
        "    ##### Update Reference  ####\n",
        "    reference_node = reference_keeper[search_data.item]\n",
        "    reference_node.sup_count += 1\n",
        "  return current_root_locator\n",
        "  \n",
        "def print_tree(tree_node) : \n",
        "  if(tree_node == None ) :\n",
        "    return\n",
        "  print(f'{tree_node.data}')\n",
        "  children = tree_node.children\n",
        "  if(len(children)==0) :\n",
        "    return\n",
        "  for child in children.values():\n",
        "    print_tree(child)\n",
        "\n",
        "def generate_fp_itemsets(root_node, reference_keeper, min_sup_cnt):\n",
        "\n",
        "  ### sort references in descending order of support count ####\n",
        "  reference_keeper = dict(sorted(reference_keeper.items(), key=lambda item: item[1].sup_count))\n",
        "\n",
        "  for (item,reference) in reference_keeper.items() :\n",
        "    if(reference.sup_count >= min_sup_cnt) :\n",
        "      reference.con_sup_count = reference.sup_count\n",
        "      itemsets = generate_conditional_fp_itemsets(root_node,list() ,min_sup_cnt, reference, reference_keeper)\n",
        "      break\n",
        "      # print(itemsets)\n",
        "   \n",
        "\n",
        "def generate_conditional_fp_itemsets(root_node, conditional_itemset, min_sup_cnt, leaf_reference, reference_keeper):\n",
        "   \n",
        "  ##################### Update conditional support counts from bottom to top ###########################\n",
        "  con_reference_keeper = set()\n",
        "  current_x_ref = leaf_reference.head\n",
        "\n",
        "  if leaf_reference.con_sup_count >= min_sup_cnt :\n",
        "    conditional_itemset = [current_x_ref.data.item, *conditional_itemset]\n",
        "    print(f'I : {conditional_itemset}')\n",
        "  else :\n",
        "    return []\n",
        "\n",
        "  ################ Reset conditional support counts ##################\n",
        "  for (item,reference) in reference_keeper.items():\n",
        "    reference.con_sup_count = 0\n",
        "\n",
        "  ##################### select prefix paths and update conditional support counts #######################\n",
        "  while(current_x_ref != None) : \n",
        "    current_x_ref.data.con_sup_cnt = 0\n",
        "    current_y_ref = current_x_ref.parent\n",
        "\n",
        "    #### Update conditional support counts through prefix paths from bottom to top ####\n",
        "    while(current_y_ref.data.item != None): #### vertical movement ####\n",
        "      con_reference_keeper.add(current_y_ref.reference)\n",
        "      current_y_ref.reference.con_sup_count += 1\n",
        "      current_y_ref.data.con_sup_cnt += 1\n",
        "      current_y_ref = current_y_ref.parent\n",
        "\n",
        "    current_x_ref = current_x_ref.next   #### Horizontal movement ####\n",
        "    \n",
        "    ###################### remove low support count nodes from the tree #################################\n",
        "    # for (key, reference) in con_reference_keeper.items():\n",
        "    #   if(reference.con_sup_cnt<min_sup_cnt):\n",
        "    #     current_pointer = reference.head \n",
        "    #     current_parent = current_pointer.parent\n",
        "    #     for child_node in current_pointer.children:\n",
        "    #       child_node.parent = current_parent\n",
        "    \n",
        "  \n",
        "\n",
        "  itemsets = []\n",
        "  for next_reference in con_reference_keeper:\n",
        "    itemsets = [*generate_conditional_fp_itemsets(root_node, conditional_itemset, min_sup_cnt,next_reference ,reference_keeper),*conditional_itemset]\n",
        "  return itemsets\n",
        "  # print(f'Conditional Reference Keeper \\n {con_reference_keeper}')\n",
        "  # next_leaf_reference = leaf_reference.head\n",
        "  # generate_fp_itemsets(root_node, conditional_itemset, min_sup_cnt, reference_keeper)\n",
        "      \n",
        "      \n",
        "      \n",
        "########################################### Testing ###################################################################\n",
        "(root_node, reference_keeper) = generate_fp_tree(min_sup_cnt = 2)\n",
        "#### Print reference chain for each frequent items ####\n",
        "for (key,value) in reference_keeper.items() :\n",
        "  print(f'Support Count of {value.head.data.item} : {value.sup_count}')\n",
        "  temp = value.head\n",
        "  while(temp != None) :\n",
        "    print(temp.data)\n",
        "    temp = temp.next\n",
        "  print()\n",
        "\n",
        "#### Generate itemsets using FP tree ####\n",
        "generate_fp_itemsets(root_node, reference_keeper, min_sup_cnt)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhtVLsVvWSJ9"
      },
      "source": [
        "References\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/08/python-tutorial-working-with-csv-file-for-data-science/\n",
        "\n",
        "https://towardsdatascience.com/magic-commands-for-profiling-in-jupyter-notebook-d2ef00e29a63\n",
        "\n",
        "http://www.btechsmartclass.com/data_structures/tree-terminology.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asktm-VbnROI",
        "outputId": "d1194f6e-b317-4d92-a15c-e0d4feed0133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of frequent itemsets :      support                itemsets\n",
            "0   0.352941               (BISCUIT)\n",
            "1   0.176471             (BOURNVITA)\n",
            "2   0.647059                 (BREAD)\n",
            "3   0.117647                  (COCK)\n",
            "4   0.352941                (COFFEE)\n",
            "5   0.294118            (CORNFLAKES)\n",
            "6   0.117647                   (JAM)\n",
            "7   0.294118                 (MAGGI)\n",
            "8   0.294118                  (MILK)\n",
            "9   0.294118                 (SUGER)\n",
            "10  0.352941                   (TEA)\n",
            "11  0.235294        (BISCUIT, BREAD)\n",
            "12  0.117647   (BISCUIT, CORNFLAKES)\n",
            "13  0.117647        (MAGGI, BISCUIT)\n",
            "14  0.117647         (MILK, BISCUIT)\n",
            "15  0.117647          (BISCUIT, TEA)\n",
            "16  0.117647      (BOURNVITA, BREAD)\n",
            "17  0.117647      (BOURNVITA, SUGER)\n",
            "18  0.117647         (COFFEE, BREAD)\n",
            "19  0.117647            (JAM, BREAD)\n",
            "20  0.176471          (MAGGI, BREAD)\n",
            "21  0.235294           (MILK, BREAD)\n",
            "22  0.176471          (SUGER, BREAD)\n",
            "23  0.176471            (TEA, BREAD)\n",
            "24  0.117647          (COFFEE, COCK)\n",
            "25  0.176471    (COFFEE, CORNFLAKES)\n",
            "26  0.176471         (COFFEE, SUGER)\n",
            "27  0.117647      (MILK, CORNFLAKES)\n",
            "28  0.117647       (CORNFLAKES, TEA)\n",
            "29  0.117647            (MAGGI, JAM)\n",
            "30  0.235294            (MAGGI, TEA)\n",
            "31  0.117647  (MILK, BISCUIT, BREAD)\n",
            "32  0.117647   (MAGGI, BISCUIT, TEA)\n",
            "33  0.117647     (MAGGI, JAM, BREAD)\n",
            "34  0.117647     (TEA, MAGGI, BREAD)\n",
            "Frequent itemsets from apriori \n",
            "    support     itemsets\n",
            "0  0.352941    (BISCUIT)\n",
            "1  0.176471  (BOURNVITA)\n",
            "2  0.647059      (BREAD)\n",
            "3  0.117647       (COCK)\n",
            "4  0.352941     (COFFEE)\n",
            "Frequent itemsets from apriori approach \n",
            "     support                itemsets\n",
            "30  0.235294            (MAGGI, TEA)\n",
            "31  0.117647  (MILK, BISCUIT, BREAD)\n",
            "32  0.117647   (MAGGI, BISCUIT, TEA)\n",
            "33  0.117647     (MAGGI, JAM, BREAD)\n",
            "34  0.117647     (TEA, MAGGI, BREAD)\n",
            "         antecedents     consequents  antecedent support  consequent support  \\\n",
            "0          (BISCUIT)         (BREAD)            0.352941            0.647059   \n",
            "1        (BOURNVITA)         (BREAD)            0.176471            0.647059   \n",
            "2        (BOURNVITA)         (SUGER)            0.176471            0.294118   \n",
            "3              (JAM)         (BREAD)            0.117647            0.647059   \n",
            "4            (MAGGI)         (BREAD)            0.294118            0.647059   \n",
            "5             (MILK)         (BREAD)            0.294118            0.647059   \n",
            "6            (SUGER)         (BREAD)            0.294118            0.647059   \n",
            "7              (TEA)         (BREAD)            0.352941            0.647059   \n",
            "8             (COCK)        (COFFEE)            0.117647            0.352941   \n",
            "9           (COFFEE)    (CORNFLAKES)            0.352941            0.294118   \n",
            "10      (CORNFLAKES)        (COFFEE)            0.294118            0.352941   \n",
            "11          (COFFEE)         (SUGER)            0.352941            0.294118   \n",
            "12           (SUGER)        (COFFEE)            0.294118            0.352941   \n",
            "13             (JAM)         (MAGGI)            0.117647            0.294118   \n",
            "14           (MAGGI)           (TEA)            0.294118            0.352941   \n",
            "15             (TEA)         (MAGGI)            0.352941            0.294118   \n",
            "16   (MILK, BISCUIT)         (BREAD)            0.117647            0.647059   \n",
            "17     (MILK, BREAD)       (BISCUIT)            0.235294            0.352941   \n",
            "18  (BISCUIT, BREAD)          (MILK)            0.235294            0.294118   \n",
            "19  (MAGGI, BISCUIT)           (TEA)            0.117647            0.352941   \n",
            "20      (MAGGI, TEA)       (BISCUIT)            0.235294            0.352941   \n",
            "21    (BISCUIT, TEA)         (MAGGI)            0.117647            0.294118   \n",
            "22      (MAGGI, JAM)         (BREAD)            0.117647            0.647059   \n",
            "23    (MAGGI, BREAD)           (JAM)            0.176471            0.117647   \n",
            "24      (JAM, BREAD)         (MAGGI)            0.117647            0.294118   \n",
            "25             (JAM)  (MAGGI, BREAD)            0.117647            0.176471   \n",
            "26      (MAGGI, TEA)         (BREAD)            0.235294            0.647059   \n",
            "27      (BREAD, TEA)         (MAGGI)            0.176471            0.294118   \n",
            "28    (MAGGI, BREAD)           (TEA)            0.176471            0.352941   \n",
            "\n",
            "     support  confidence      lift  leverage  conviction  \n",
            "0   0.235294    0.666667  1.030303  0.006920    1.058824  \n",
            "1   0.117647    0.666667  1.030303  0.003460    1.058824  \n",
            "2   0.117647    0.666667  2.266667  0.065744    2.117647  \n",
            "3   0.117647    1.000000  1.545455  0.041522         inf  \n",
            "4   0.176471    0.600000  0.927273 -0.013841    0.882353  \n",
            "5   0.235294    0.800000  1.236364  0.044983    1.764706  \n",
            "6   0.176471    0.600000  0.927273 -0.013841    0.882353  \n",
            "7   0.176471    0.500000  0.772727 -0.051903    0.705882  \n",
            "8   0.117647    1.000000  2.833333  0.076125         inf  \n",
            "9   0.176471    0.500000  1.700000  0.072664    1.411765  \n",
            "10  0.176471    0.600000  1.700000  0.072664    1.617647  \n",
            "11  0.176471    0.500000  1.700000  0.072664    1.411765  \n",
            "12  0.176471    0.600000  1.700000  0.072664    1.617647  \n",
            "13  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "14  0.235294    0.800000  2.266667  0.131488    3.235294  \n",
            "15  0.235294    0.666667  2.266667  0.131488    2.117647  \n",
            "16  0.117647    1.000000  1.545455  0.041522         inf  \n",
            "17  0.117647    0.500000  1.416667  0.034602    1.294118  \n",
            "18  0.117647    0.500000  1.700000  0.048443    1.411765  \n",
            "19  0.117647    1.000000  2.833333  0.076125         inf  \n",
            "20  0.117647    0.500000  1.416667  0.034602    1.294118  \n",
            "21  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "22  0.117647    1.000000  1.545455  0.041522         inf  \n",
            "23  0.117647    0.666667  5.666667  0.096886    2.647059  \n",
            "24  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "25  0.117647    1.000000  5.666667  0.096886         inf  \n",
            "26  0.117647    0.500000  0.772727 -0.034602    0.705882  \n",
            "27  0.117647    0.666667  2.266667  0.065744    2.117647  \n",
            "28  0.117647    0.666667  1.888889  0.055363    1.941176  \n",
            "\n",
            "\n",
            "Shape of association rules : (29, 9)\n",
            "Association rules(Head) \n",
            "   antecedents consequents  antecedent support  consequent support   support  \\\n",
            "0    (BISCUIT)     (BREAD)            0.352941            0.647059  0.235294   \n",
            "1  (BOURNVITA)     (BREAD)            0.176471            0.647059  0.117647   \n",
            "2  (BOURNVITA)     (SUGER)            0.176471            0.294118  0.117647   \n",
            "3        (JAM)     (BREAD)            0.117647            0.647059  0.117647   \n",
            "4      (MAGGI)     (BREAD)            0.294118            0.647059  0.176471   \n",
            "\n",
            "   confidence      lift  leverage  conviction  \n",
            "0    0.666667  1.030303  0.006920    1.058824  \n",
            "1    0.666667  1.030303  0.003460    1.058824  \n",
            "2    0.666667  2.266667  0.065744    2.117647  \n",
            "3    1.000000  1.545455  0.041522         inf  \n",
            "4    0.600000  0.927273 -0.013841    0.882353  \n",
            "Association rules(Tail) \n",
            "       antecedents     consequents  antecedent support  consequent support  \\\n",
            "24    (JAM, BREAD)         (MAGGI)            0.117647            0.294118   \n",
            "25           (JAM)  (MAGGI, BREAD)            0.117647            0.176471   \n",
            "26    (MAGGI, TEA)         (BREAD)            0.235294            0.647059   \n",
            "27    (BREAD, TEA)         (MAGGI)            0.176471            0.294118   \n",
            "28  (MAGGI, BREAD)           (TEA)            0.176471            0.352941   \n",
            "\n",
            "     support  confidence      lift  leverage  conviction  \n",
            "24  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "25  0.117647    1.000000  5.666667  0.096886         inf  \n",
            "26  0.117647    0.500000  0.772727 -0.034602    0.705882  \n",
            "27  0.117647    0.666667  2.266667  0.065744    2.117647  \n",
            "28  0.117647    0.666667  1.888889  0.055363    1.941176  \n"
          ]
        }
      ],
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "#Let's transform the list, with one-hot encoding\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "a = TransactionEncoder()\n",
        "a_data = a.fit(database).transform(database)\n",
        "df = pd.DataFrame(a_data,columns=a.columns_)\n",
        "df = df.replace(False,0)\n",
        "df\n",
        "\n",
        "\n",
        "df = apriori(df, min_support = 0.1, use_colnames = True)\n",
        "print(f'Shape of frequent itemsets : {df}')\n",
        "print(f'Frequent itemsets from apriori \\n{df.head()}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{df.tail()}')\n",
        "\n",
        "df_ar = association_rules(df, metric = \"confidence\", min_threshold = 0.5)\n",
        "print(df_ar)\n",
        "print(f'\\n\\nShape of association rules : {df_ar.shape}')\n",
        "print(f'Association rules(Head) \\n{df_ar.head()}')\n",
        "print(f'Association rules(Tail) \\n{df_ar.tail()}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Associated Rule Mining.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbDfaTPPx+N73k2B/JeAEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
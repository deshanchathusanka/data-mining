{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Associated Rule Mining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH78x0wdQxPJ",
        "outputId": "788ba07e-d397-4fc1-f31e-b0f1e524cf45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting line_profiler\n",
            "  Downloading line_profiler-3.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: line-profiler\n",
            "Successfully installed line-profiler-3.5.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Small\" \"/content/\"\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Large\" \"/content/\"\n",
        "!pip3 install line_profiler\n",
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "b5u30WFOTpxp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_from_file(file_name) :\n",
        "  csvfile = open(file = file_name, mode = 'r') # open file\n",
        "  print(type(csvfile))\n",
        "\n",
        "  csvreader = csv.reader(csvfile, skipinitialspace = True) # csv reader object\n",
        "  rows = [] # empty list\n",
        "  for row in csvreader:\n",
        "    rows.append(row)\n",
        "\n",
        "  csvfile.close() #' close file\n",
        "  return rows\n",
        "\n",
        "def write_to_file(file_name, data) :\n",
        "  file = open(file = file_name, mode = 'w') # file object\n",
        "\n",
        "  csvwriter = csv.writer(file) # csv reader object\n",
        "  for row in data:\n",
        "    csvwriter.writerow(row)\n",
        "\n",
        "  file.close() # close file"
      ],
      "metadata": {
        "id": "c6KxqY0GTQBh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = read_from_file('Dataset.Small/GroceryStore.csv') # read transactions or database\n",
        "write_to_file('Transactions.csv', transactions) # write transactions or database into another file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VesjogsS1upZ",
        "outputId": "1187cfbe-121e-4d82-bd4d-ba1eaf68efe4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '_io.TextIOWrapper'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Brute-Force Approach(Frequent itemset mining)**\n",
        "\n",
        "$\n",
        "Number\\ of\\ unique\\ items\\ =\\ d\\\\\n",
        "Number\\ of\\ transactions\\ =\\ N\\\\\n",
        "Average\\ width\\ of\\ a\\ transaction=\\ w\\\\ \\\\\n",
        "Number\\ of\\ all\\ possible\\ combinations\\ =\\\n",
        "\\displaystyle\\sum_{r=1} ^{d} c_{r}\\ =\\ 2^d - 1\\\\\n",
        "Complexity\\ =\\ O(Nw2^{d})\n",
        "$"
      ],
      "metadata": {
        "id": "hiiyEGq5r1Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_database(database, itemset) :\n",
        "  \"\"\"\n",
        "    Search occurance of given itemset in the transaction database\n",
        "    Inputs\n",
        "      database : Transactions : List of lists \n",
        "      itemset : Itemset that should be searched : List\n",
        "    Outputs\n",
        "      occurance : Number of occurance : integer\n",
        "  \"\"\"\n",
        "  frequency = np.count_nonzero([all(item in transaction for item in itemset) for transaction in database])\n",
        "  return frequency\n",
        "\n",
        "def calc_candidate_sup_cnt(database, c_k, k) : \n",
        "  \"\"\"\n",
        "  Calculate support count for all candidate k-itemsets and generate dataframe\n",
        "  Inputs\n",
        "    database : Transactions : List of lists\n",
        "    c_k : Candidate k-itemsets : List of tuples\n",
        "  Output : \n",
        "    c_k_df : Candidate k-itemsets dataframe with support counts\n",
        "  \"\"\"\n",
        "  itemset_df = pd.DataFrame(columns=['key', 'item_cnt','itemset','support_cnt'])\n",
        "  for itemset in c_k: # itemset : tuple of items\n",
        "      support_count = search_database(database, itemset)\n",
        "      data_entry = {\n",
        "        \"key\" : frozenset(itemset), # immutable set\n",
        "        \"item_cnt\": k,\n",
        "        \"itemset\": itemset,\n",
        "        \"support_cnt\": support_count\n",
        "      }\n",
        "      itemset_df = itemset_df.append(data_entry, ignore_index = True)\n",
        "  return itemset_df\n",
        "\n",
        "def select_freq_itemsets(c_k, min_sup_cnt) :\n",
        "  \"\"\"\n",
        "  Select frequent k-itemsets from candidate k-itemsets(Candidate elimination)\n",
        "  Inputs\n",
        "    c_k : Candidate k-itemsets : DataFrame(itemset,support_cnt)\n",
        "    min_sup_cnt : minimum support count(hyper parameter) : integer\n",
        "  Outputs\n",
        "    f_k : Frequent k-itemsets : DataFrame(itemset,support_cnt)\n",
        "  \"\"\"\n",
        "  f_k = c_k[c_k['support_cnt'] >= min_sup_cnt] \n",
        "  return f_k\n",
        "\n",
        "def brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups):\n",
        "  \"\"\" \n",
        "  Brute Force Method \n",
        "  Inputs\n",
        "    database : Transaction database : List of lists\n",
        "    unique_items : Unique items in the database : List of items\n",
        "    min_sup_cnt : Minimum support count : intereger\n",
        "    itemset_groups : Required item groups(k values)\n",
        "  Outputs\n",
        "    all_freq_itemsets = Frequent itemsets for given groups = DataFrame\n",
        "  \"\"\"\n",
        "  all_candidate_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups: # loop number of itemset groups\n",
        "    c_k = combinations(unique_items, k) # create combinations for each itemset group\n",
        "    c_k_df = calc_candidate_sup_cnt(database, c_k, k)\n",
        "    all_candidate_df = all_candidate_df.append(c_k_df, ignore_index = True)\n",
        "\n",
        "  all_freq_itemsets = select_freq_itemsets(all_candidate_df, min_sup_cnt)\n",
        "  return all_freq_itemsets"
      ],
      "metadata": {
        "id": "nxQWILyHvSVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Small dataset\"\"\"\n",
        "transaction_df = pd.read_csv('Dataset.Small/GroceryStore.csv', header=None)\n",
        "transaction_df.drop_duplicates(subset =0,\n",
        "                     keep = False, inplace = True)\n",
        "database = list(transaction_df[0].apply(lambda x:x.split(\",\") ))\n",
        "print(f'Size of transactions : {len(database)}')\n",
        "\n",
        "unique_items = set()\n",
        "for transaction in database:\n",
        "  for item in transaction :\n",
        "    unique_items.add(item)\n",
        "# print(unique_items)\n",
        "\n",
        "\n",
        "\"\"\" Large dataset \"\"\"\n",
        "transaction_df_large = pd.read_csv('Dataset.Large/OnlineRetail.csv', header=None)\n",
        "print(f'Size of large dataset : {transaction_df_large.shape}')\n",
        "\n",
        "min_sup_cnt = 2\n",
        "itemset_groups = [1, 2, 3, 4, 5]\n",
        "freq_itemsets = brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups)\n",
        "print(f'\\n\\nShape of frequent itemsets : {freq_itemsets.shape}')\n",
        "print(f'Frequent itemsets from brute-force approach \\n{freq_itemsets.head()}')\n",
        "print(f'Frequent itemsets from brute-force approach \\n{freq_itemsets.tail()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcLQWK_7lej",
        "outputId": "f47dcabc-25e4-4fb2-ae15-1fd02deeff47"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of transactions : 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (3,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of large dataset : (541910, 8)\n",
            "\n",
            "\n",
            "Shape of frequent itemsets : (30, 4)\n",
            "Frequent itemsets from brute-force approach \n",
            "  item_cnt       itemset support_cnt          key\n",
            "0        1      (MAGGI,)           5      (MAGGI)\n",
            "1        1      (SUGER,)           4      (SUGER)\n",
            "2        1    (BISCUIT,)           5    (BISCUIT)\n",
            "3        1  (BOURNVITA,)           2  (BOURNVITA)\n",
            "4        1        (JAM,)           2        (JAM)\n",
            "Frequent itemsets from brute-force approach \n",
            "    item_cnt                 itemset support_cnt                     key\n",
            "62         2           (MILK, BREAD)           4           (MILK, BREAD)\n",
            "78         3   (MAGGI, BISCUIT, TEA)           2   (MAGGI, BISCUIT, TEA)\n",
            "95         3     (MAGGI, JAM, BREAD)           2     (MAGGI, BREAD, JAM)\n",
            "104        3     (MAGGI, TEA, BREAD)           2     (MAGGI, BREAD, TEA)\n",
            "171        3  (BISCUIT, MILK, BREAD)           2  (MILK, BREAD, BISCUIT)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apriori Algorithm(Frequent itemset mining)**\n",
        "\n",
        "* Superset is frequent >>> Subset is frequent (Bottom Up)\n",
        "\n",
        "* Sebset is infrequent >>> Superset is infrequent (Top down)\n",
        "\n",
        "* This algorithm has **anti-monotone** propety >>> support of itemset can not exceed support of its subset\n",
        "\n",
        "\n",
        "**Main Steps**\n",
        "\n",
        "* Candidate Generation ($F_{k-1} >> C_{k}$)\n",
        "\n",
        "* Candidate Pruning\n",
        "\n",
        "* Support Counting\n",
        "\n",
        "* Candidate Elimination ($C_{k} >> F_{k}$)\n",
        "\n",
        "**Candidate Pruning ($F_{k-1}×F_{k-1}$)**\n",
        "\n",
        "$Itemset\\ is\\ not\\ frequent\\ if\\ one\\ of\\ its\\ sub\\ itemset\\ is\\ not\\ frequent$\n",
        "\n",
        "* Number of **(k-1)-size subsets** for a **k-itemset** = $C^{K}_{K-1}$ = $K$\n",
        "* Number of **already verified (k-1)-size subsets** at the candidate generation = $2$\n",
        "* Number of subsets for frequency verification stage per each **k-itemset** = $K-2$\n",
        "* Total number of subsets for frequency verification for **all k-itemsets** = $L_{k}\\times(K-2)$"
      ],
      "metadata": {
        "id": "lUYANEetVox9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(f_prv, k):\n",
        "  \"\"\"\n",
        "  Generate candidate set for k-itemsets\n",
        "  Input : \n",
        "    f_prv : F(K-1) \n",
        "  Output : \n",
        "    c_k : List of candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  c_k = [] # candidate k-itemset\n",
        "  f_prv_pairs = combinations(f_prv, 2) # select pair of frequent (k-1)-itemsets\n",
        "  for f_prv_pair in f_prv_pairs:\n",
        "    if(len(f_prv_pair[0]) == 1) : ### K=2 ###\n",
        "      two_itemset = [*f_prv_pair[0],*f_prv_pair[1]]\n",
        "      two_itemset.sort()         \n",
        "      c_k.append(two_itemset)\n",
        "    elif(f_prv_pair[0][0:k-2] == f_prv_pair[1][0:k-2]) :### K>=2 and first (k-2) items are same in both itemsets ###\n",
        "      generated_itemset = f_prv_pair[0][0:k-2]\n",
        "      delta_items = [f_prv_pair[0][k-2],f_prv_pair[1][k-2]]\n",
        "      delta_items.sort()\n",
        "      generated_itemset = [*generated_itemset,*delta_items]\n",
        "      c_k.append(generated_itemset)\n",
        "  return c_k\n",
        "\n",
        "def prune_candidates(c_k, f_prv, k) :\n",
        "  \"\"\"\n",
        "  Prune candidate set from k-itemsets\n",
        "  Inputs\n",
        "    c_k : candidate k-itemsets : List of lists\n",
        "    f_prv : frequent (k-1)-itemsets \n",
        "  Output\n",
        "    c_k_prune : pruned candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  infrq_itemsets = []\n",
        "  ### search candidate k-itemsets to check frequency of its (k-1)-sized subsets\n",
        "  for itemset in c_k :\n",
        "    subsets = combinations(itemset, k-1)\n",
        "    for subset in subsets:\n",
        "      frequency = search_database(f_prv, subset)\n",
        "      if(frequency==0):\n",
        "        infrq_itemsets.append(itemset)\n",
        "        break\n",
        "  ### remove infrequent itemsets from candidate k-itemsets ###\n",
        "  for itemset in infrq_itemsets :\n",
        "    c_k.remove(itemset)\n",
        "\n",
        "  return c_k\n"
      ],
      "metadata": {
        "id": "Ufd1lKSaVv6K"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori_algorithm(min_sup_cnt) :\n",
        "  f_itemsets_store = {}\n",
        "  all_frq_itemsets_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups:\n",
        "    if(k>1) :\n",
        "      f_prv = f_itemsets_store[k-1]['itemset']\n",
        "      c_k = generate_candidates(f_prv, k) # generate candidates\n",
        "      c_k = prune_candidates(c_k, f_prv, k) # prune candidates\n",
        "      c_k = calc_candidate_sup_cnt(database, c_k, k)\n",
        "      f_k = select_freq_itemsets(c_k, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_k, ignore_index=True)\n",
        "      f_itemsets_store[k] = f_k\n",
        "    else :\n",
        "      c_1 = [[item] for item in unique_items] # candidate 1-itemset\n",
        "      c_1 = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "      f_1 = select_freq_itemsets(c_1, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_1, ignore_index=True)\n",
        "      f_itemsets_store[1] = f_1\n",
        "  return all_frq_itemsets_df, f_itemsets_store\n",
        "\n",
        "# all_itemsets_df = %lprun -f apriori_algorithm apriori_algorithm() # execute line profiler\n",
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2) # execute line profiler\n",
        "print(f'Shape of frequent itemsets : {all_frq_itemsets_df.shape}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{all_frq_itemsets_df.head()}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{all_frq_itemsets_df.tail()}')\n"
      ],
      "metadata": {
        "id": "GnYQvEz__3Nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6dd7ae-5283-40f3-b770-af3a7012174e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of frequent itemsets : (30, 4)\n",
            "Frequent itemsets from apriori approach \n",
            "  item_cnt      itemset support_cnt          key\n",
            "0        1      [MAGGI]           5      (MAGGI)\n",
            "1        1      [SUGER]           4      (SUGER)\n",
            "2        1    [BISCUIT]           5    (BISCUIT)\n",
            "3        1  [BOURNVITA]           2  (BOURNVITA)\n",
            "4        1        [JAM]           2        (JAM)\n",
            "Frequent itemsets from apriori approach \n",
            "   item_cnt                 itemset support_cnt                     key\n",
            "25        2           [BREAD, MILK]           4           (MILK, BREAD)\n",
            "26        3   [BISCUIT, MAGGI, TEA]           2   (MAGGI, BISCUIT, TEA)\n",
            "27        3     [BREAD, JAM, MAGGI]           2     (MAGGI, BREAD, JAM)\n",
            "28        3     [BREAD, MAGGI, TEA]           2     (MAGGI, BREAD, TEA)\n",
            "29        3  [BISCUIT, BREAD, MILK]           2  (MILK, BREAD, BISCUIT)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rule Generation**\n",
        "\n",
        "Antecedent -> Consequent\n",
        "\n",
        "Ex : {X,Y} -> {P,Q}"
      ],
      "metadata": {
        "id": "gDqu5cMCV8LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rules(f_k, h_m, f_itemsets_store, min_conf) :\n",
        "  \"\"\"\n",
        "  Generate rules from frequent itemsets\n",
        "  Inputs:\n",
        "    f_k : Frequent k-itemsets (X,Y,P,Q) : Tuple\n",
        "    h_m : Consequent (P,Q) : Tuple\n",
        "    f_itemsets_store : Frequent itemset store(key = k, value = support counts) : Dictionary(Key = Integer, value = DataFrame)\n",
        "  \"\"\"\n",
        "  conf = 0\n",
        "  if(len(h_m) > 0) :\n",
        "    k = len(f_k)\n",
        "    m = len(h_m[0])\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "  \n",
        "  if(k > m+1) : #### Check whether an item can be passed from antecedent to consequent >>> Terminating condition of the recursive function###\n",
        "    h_next = generate_candidates(h_m, m+1) # Generate (m+1) candidates for consequent\n",
        "    h_next = prune_candidates(h_next, h_m, m+1) # Prune (m+1) candidates\n",
        "    h_next_invalid = []\n",
        "    rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "    for h_next_i in h_next : # Iterate candidate consequents\n",
        "      consequent = h_next_i\n",
        "      antecedent = list(set(f_k) - set(h_next_i))\n",
        "      rule_sup_cnt = f_itemsets_store[k].loc[f_itemsets_store[k]['key']==frozenset(f_k)]\n",
        "      antecedent_sup_cnt = f_itemsets_store[len(antecedent)].loc[f_itemsets_store[len(antecedent)]['key']==frozenset(antecedent)]\n",
        "\n",
        "      if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "        conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "      if(conf > min_conf) : \n",
        "        data_entry = {\n",
        "          'antecedent': antecedent,\n",
        "          'consequent': consequent,\n",
        "          'confidence': conf\n",
        "        }\n",
        "        rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "      else : \n",
        "        h_next_invalid.append(h_next_i)\n",
        "\n",
        "    for h_next_invalid_i in h_next_invalid : # Remove invalid consequents \n",
        "      h_next.remove(h_next_invalid_i)\n",
        "\n",
        "    ### recursively generate rules for subgraphs \n",
        "    return generate_rules(f_k, h_next, f_itemsets_store, min_conf).append(rule_df, ignore_index=True)\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])"
      ],
      "metadata": {
        "id": "JglQrjGLWedO"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2)\n",
        "min_conf = 0.5\n",
        "print(all_frq_itemsets_df.shape)\n",
        "\n",
        "rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "for f_k in all_frq_itemsets_df['itemset'] :\n",
        "  h_1_invalid = []\n",
        "  for item in f_k :\n",
        "    if len(f_k)==1 :\n",
        "      continue\n",
        "    consequent = []\n",
        "    consequent.append(item)\n",
        "    antecedent = list(set(f_k)-set(consequent))\n",
        "\n",
        "    rule_sup_cnt = f_itemsets_store[len(f_k)].loc[f_itemsets_store[len(f_k)]['key']==frozenset(f_k)]\n",
        "    antecedent_sup_cnt = f_itemsets_store[len(antecedent)].loc[f_itemsets_store[len(antecedent)]['key']==frozenset(antecedent)]\n",
        "\n",
        "    if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "      conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "    if(conf >= min_conf) : \n",
        "      data_entry = {\n",
        "        'antecedent': antecedent,\n",
        "        'consequent': consequent,\n",
        "        'confidence': conf\n",
        "      }\n",
        "      rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "    else : \n",
        "      h_1_invalid.append(consequent)\n",
        "\n",
        "  h_1 = [[item] for item in f_k] # create consequents for single items\n",
        "  if len(h_1_invalid)>0 : # remove low confidence items from consequents\n",
        "    for h_1_invalid_i in h_1_invalid : # Remove invalid consequents \n",
        "      h_1.remove(h_1_invalid_i)\n",
        "  if len(h_1)==0 :\n",
        "    continue\n",
        "    \n",
        "  rule_df = rule_df.append(generate_rules(f_k, h_1, f_itemsets_store, min_conf), ignore_index=True)\n",
        "\n",
        "print(rule_df)\n",
        "print(f'\\n\\nShape of association rules : {rule_df.shape}')\n",
        "print(f'Association rules(Head) \\n{rule_df.head()}')\n",
        "print(f'Association rules(Tail) \\n{rule_df.tail()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l5XsaZHP9m0",
        "outputId": "57abeb40-3868-450b-a922-533e4938094f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 4)\n",
            "          antecedent      consequent  confidence\n",
            "0              [JAM]         [MAGGI]    1.000000\n",
            "1              [TEA]         [MAGGI]    0.800000\n",
            "2            [MAGGI]           [TEA]    0.800000\n",
            "3            [MAGGI]         [BREAD]    0.600000\n",
            "4            [SUGER]     [BOURNVITA]    0.500000\n",
            "5        [BOURNVITA]         [SUGER]    1.000000\n",
            "6            [SUGER]        [COFFEE]    0.500000\n",
            "7           [COFFEE]         [SUGER]    0.500000\n",
            "8            [SUGER]         [BREAD]    0.500000\n",
            "9          [BISCUIT]         [BREAD]    0.800000\n",
            "10             [JAM]         [BREAD]    1.000000\n",
            "11      [CORNFLAKES]           [TEA]    0.500000\n",
            "12      [CORNFLAKES]          [MILK]    0.500000\n",
            "13      [CORNFLAKES]        [COFFEE]    0.500000\n",
            "14          [COFFEE]    [CORNFLAKES]    0.500000\n",
            "15            [MILK]         [BREAD]    0.800000\n",
            "16      [MAGGI, TEA]       [BISCUIT]    0.500000\n",
            "17    [BISCUIT, TEA]         [MAGGI]    1.000000\n",
            "18  [MAGGI, BISCUIT]           [TEA]    1.000000\n",
            "19      [MAGGI, JAM]         [BREAD]    1.000000\n",
            "20    [MAGGI, BREAD]           [JAM]    0.666667\n",
            "21      [BREAD, JAM]         [MAGGI]    1.000000\n",
            "22             [JAM]  [BREAD, MAGGI]    1.000000\n",
            "23      [MAGGI, TEA]         [BREAD]    0.500000\n",
            "24      [BREAD, TEA]         [MAGGI]    1.000000\n",
            "25    [MAGGI, BREAD]           [TEA]    0.666667\n",
            "26     [MILK, BREAD]       [BISCUIT]    0.500000\n",
            "27   [MILK, BISCUIT]         [BREAD]    1.000000\n",
            "28  [BREAD, BISCUIT]          [MILK]    0.500000\n",
            "\n",
            "\n",
            "Shape of association rules : (29, 3)\n",
            "Association rules(Head) \n",
            "  antecedent   consequent  confidence\n",
            "0      [JAM]      [MAGGI]         1.0\n",
            "1      [TEA]      [MAGGI]         0.8\n",
            "2    [MAGGI]        [TEA]         0.8\n",
            "3    [MAGGI]      [BREAD]         0.6\n",
            "4    [SUGER]  [BOURNVITA]         0.5\n",
            "Association rules(Tail) \n",
            "          antecedent consequent  confidence\n",
            "24      [BREAD, TEA]    [MAGGI]    1.000000\n",
            "25    [MAGGI, BREAD]      [TEA]    0.666667\n",
            "26     [MILK, BREAD]  [BISCUIT]    0.500000\n",
            "27   [MILK, BISCUIT]    [BREAD]    1.000000\n",
            "28  [BREAD, BISCUIT]     [MILK]    0.500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FP-Growth Algorithm**"
      ],
      "metadata": {
        "id": "S3kK-iwgTmIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Item:\n",
        "  def __init__(self, item, sup_count):\n",
        "    self.item = item\n",
        "    self.sup_count = sup_count\n",
        "\n",
        "class Node:\n",
        "  def __init__(self, data):\n",
        "      self.parent = None\n",
        "      self.childrens = None\n",
        "      self.data = data\n"
      ],
      "metadata": {
        "id": "dVp5lYHcX7sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fp_algorithm(min_sup_cnt):\n",
        "  c_1 = [[item] for item in unique_items]\n",
        "  itemset_df = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "  print(itemset_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ6oD_scdMKv",
        "outputId": "cb20760a-a641-4850-83df-a4583f3c3d31"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             key item_cnt       itemset support_cnt\n",
            "0        (MAGGI)        1       [MAGGI]           5\n",
            "1        (SUGER)        1       [SUGER]           4\n",
            "2      (BISCUIT)        1     [BISCUIT]           5\n",
            "3    (BOURNVITA)        1   [BOURNVITA]           2\n",
            "4          (JAM)        1         [JAM]           2\n",
            "5   (CORNFLAKES)        1  [CORNFLAKES]           4\n",
            "6          (TEA)        1         [TEA]           5\n",
            "7         (MILK)        1        [MILK]           5\n",
            "8         (COCK)        1        [COCK]           1\n",
            "9       (COFFEE)        1      [COFFEE]           4\n",
            "10       (BREAD)        1       [BREAD]           9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/08/python-tutorial-working-with-csv-file-for-data-science/\n",
        "\n",
        "https://towardsdatascience.com/magic-commands-for-profiling-in-jupyter-notebook-d2ef00e29a63\n",
        "\n",
        "http://www.btechsmartclass.com/data_structures/tree-terminology.html"
      ],
      "metadata": {
        "id": "xhtVLsVvWSJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "#Let's transform the list, with one-hot encoding\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "a = TransactionEncoder()\n",
        "a_data = a.fit(database).transform(database)\n",
        "df = pd.DataFrame(a_data,columns=a.columns_)\n",
        "df = df.replace(False,0)\n",
        "df\n",
        "\n",
        "\n",
        "df = apriori(df, min_support = 0.1, use_colnames = True)\n",
        "print(f'Shape of frequent itemsets : {df.shape}')\n",
        "print(f'Frequent itemsets from apriori \\n{df.head()}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{df.tail()}')\n",
        "\n",
        "df_ar = association_rules(df, metric = \"confidence\", min_threshold = 0.5)\n",
        "print(df_ar)\n",
        "print(f'\\n\\nShape of association rules : {df_ar.shape}')\n",
        "print(f'Association rules(Head) \\n{df_ar.head()}')\n",
        "print(f'Association rules(Tail) \\n{df_ar.tail()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asktm-VbnROI",
        "outputId": "70562af1-1403-48ff-df7d-1378fa3af04a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of frequent itemsets : (30, 2)\n",
            "Frequent itemsets from apriori \n",
            "    support      itemsets\n",
            "0  0.357143     (BISCUIT)\n",
            "1  0.142857   (BOURNVITA)\n",
            "2  0.642857       (BREAD)\n",
            "3  0.285714      (COFFEE)\n",
            "4  0.285714  (CORNFLAKES)\n",
            "Frequent itemsets from apriori approach \n",
            "     support                itemsets\n",
            "25  0.285714            (MAGGI, TEA)\n",
            "26  0.142857  (MILK, BREAD, BISCUIT)\n",
            "27  0.142857   (MAGGI, BISCUIT, TEA)\n",
            "28  0.142857     (MAGGI, BREAD, JAM)\n",
            "29  0.142857     (MAGGI, BREAD, TEA)\n",
            "         antecedents     consequents  antecedent support  consequent support  \\\n",
            "0          (BISCUIT)         (BREAD)            0.357143            0.642857   \n",
            "1        (BOURNVITA)         (SUGER)            0.142857            0.285714   \n",
            "2            (SUGER)     (BOURNVITA)            0.285714            0.142857   \n",
            "3              (JAM)         (BREAD)            0.142857            0.642857   \n",
            "4            (MAGGI)         (BREAD)            0.357143            0.642857   \n",
            "5             (MILK)         (BREAD)            0.357143            0.642857   \n",
            "6            (SUGER)         (BREAD)            0.285714            0.642857   \n",
            "7           (COFFEE)    (CORNFLAKES)            0.285714            0.285714   \n",
            "8       (CORNFLAKES)        (COFFEE)            0.285714            0.285714   \n",
            "9           (COFFEE)         (SUGER)            0.285714            0.285714   \n",
            "10           (SUGER)        (COFFEE)            0.285714            0.285714   \n",
            "11      (CORNFLAKES)          (MILK)            0.285714            0.357143   \n",
            "12      (CORNFLAKES)           (TEA)            0.285714            0.357143   \n",
            "13             (JAM)         (MAGGI)            0.142857            0.357143   \n",
            "14           (MAGGI)           (TEA)            0.357143            0.357143   \n",
            "15             (TEA)         (MAGGI)            0.357143            0.357143   \n",
            "16     (MILK, BREAD)       (BISCUIT)            0.285714            0.357143   \n",
            "17   (MILK, BISCUIT)         (BREAD)            0.142857            0.642857   \n",
            "18  (BREAD, BISCUIT)          (MILK)            0.285714            0.357143   \n",
            "19  (MAGGI, BISCUIT)           (TEA)            0.142857            0.357143   \n",
            "20      (MAGGI, TEA)       (BISCUIT)            0.285714            0.357143   \n",
            "21    (BISCUIT, TEA)         (MAGGI)            0.142857            0.357143   \n",
            "22    (MAGGI, BREAD)           (JAM)            0.214286            0.142857   \n",
            "23      (MAGGI, JAM)         (BREAD)            0.142857            0.642857   \n",
            "24      (BREAD, JAM)         (MAGGI)            0.142857            0.357143   \n",
            "25             (JAM)  (MAGGI, BREAD)            0.142857            0.214286   \n",
            "26    (MAGGI, BREAD)           (TEA)            0.214286            0.357143   \n",
            "27      (MAGGI, TEA)         (BREAD)            0.285714            0.642857   \n",
            "28      (BREAD, TEA)         (MAGGI)            0.142857            0.357143   \n",
            "\n",
            "     support  confidence      lift  leverage  conviction  \n",
            "0   0.285714    0.800000  1.244444  0.056122    1.785714  \n",
            "1   0.142857    1.000000  3.500000  0.102041         inf  \n",
            "2   0.142857    0.500000  3.500000  0.102041    1.714286  \n",
            "3   0.142857    1.000000  1.555556  0.051020         inf  \n",
            "4   0.214286    0.600000  0.933333 -0.015306    0.892857  \n",
            "5   0.285714    0.800000  1.244444  0.056122    1.785714  \n",
            "6   0.142857    0.500000  0.777778 -0.040816    0.714286  \n",
            "7   0.142857    0.500000  1.750000  0.061224    1.428571  \n",
            "8   0.142857    0.500000  1.750000  0.061224    1.428571  \n",
            "9   0.142857    0.500000  1.750000  0.061224    1.428571  \n",
            "10  0.142857    0.500000  1.750000  0.061224    1.428571  \n",
            "11  0.142857    0.500000  1.400000  0.040816    1.285714  \n",
            "12  0.142857    0.500000  1.400000  0.040816    1.285714  \n",
            "13  0.142857    1.000000  2.800000  0.091837         inf  \n",
            "14  0.285714    0.800000  2.240000  0.158163    3.214286  \n",
            "15  0.285714    0.800000  2.240000  0.158163    3.214286  \n",
            "16  0.142857    0.500000  1.400000  0.040816    1.285714  \n",
            "17  0.142857    1.000000  1.555556  0.051020         inf  \n",
            "18  0.142857    0.500000  1.400000  0.040816    1.285714  \n",
            "19  0.142857    1.000000  2.800000  0.091837         inf  \n",
            "20  0.142857    0.500000  1.400000  0.040816    1.285714  \n",
            "21  0.142857    1.000000  2.800000  0.091837         inf  \n",
            "22  0.142857    0.666667  4.666667  0.112245    2.571429  \n",
            "23  0.142857    1.000000  1.555556  0.051020         inf  \n",
            "24  0.142857    1.000000  2.800000  0.091837         inf  \n",
            "25  0.142857    1.000000  4.666667  0.112245         inf  \n",
            "26  0.142857    0.666667  1.866667  0.066327    1.928571  \n",
            "27  0.142857    0.500000  0.777778 -0.040816    0.714286  \n",
            "28  0.142857    1.000000  2.800000  0.091837         inf  \n",
            "\n",
            "\n",
            "Shape of association rules : (29, 9)\n",
            "Association rules(Head) \n",
            "   antecedents  consequents  antecedent support  consequent support   support  \\\n",
            "0    (BISCUIT)      (BREAD)            0.357143            0.642857  0.285714   \n",
            "1  (BOURNVITA)      (SUGER)            0.142857            0.285714  0.142857   \n",
            "2      (SUGER)  (BOURNVITA)            0.285714            0.142857  0.142857   \n",
            "3        (JAM)      (BREAD)            0.142857            0.642857  0.142857   \n",
            "4      (MAGGI)      (BREAD)            0.357143            0.642857  0.214286   \n",
            "\n",
            "   confidence      lift  leverage  conviction  \n",
            "0         0.8  1.244444  0.056122    1.785714  \n",
            "1         1.0  3.500000  0.102041         inf  \n",
            "2         0.5  3.500000  0.102041    1.714286  \n",
            "3         1.0  1.555556  0.051020         inf  \n",
            "4         0.6  0.933333 -0.015306    0.892857  \n",
            "Association rules(Tail) \n",
            "       antecedents     consequents  antecedent support  consequent support  \\\n",
            "24    (BREAD, JAM)         (MAGGI)            0.142857            0.357143   \n",
            "25           (JAM)  (MAGGI, BREAD)            0.142857            0.214286   \n",
            "26  (MAGGI, BREAD)           (TEA)            0.214286            0.357143   \n",
            "27    (MAGGI, TEA)         (BREAD)            0.285714            0.642857   \n",
            "28    (BREAD, TEA)         (MAGGI)            0.142857            0.357143   \n",
            "\n",
            "     support  confidence      lift  leverage  conviction  \n",
            "24  0.142857    1.000000  2.800000  0.091837         inf  \n",
            "25  0.142857    1.000000  4.666667  0.112245         inf  \n",
            "26  0.142857    0.666667  1.866667  0.066327    1.928571  \n",
            "27  0.142857    0.500000  0.777778 -0.040816    0.714286  \n",
            "28  0.142857    1.000000  2.800000  0.091837         inf  \n"
          ]
        }
      ]
    }
  ]
}
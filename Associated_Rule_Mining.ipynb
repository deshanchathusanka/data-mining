{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deshanchathusanka/data-mining/blob/main/Associated_Rule_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH78x0wdQxPJ",
        "outputId": "ec582315-e53a-46fd-d9e1-495104dac792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/Dataset.Small': File exists\n",
            "ln: failed to create symbolic link '/content/Dataset.Large': File exists\n",
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.7/dist-packages (3.5.1)\n",
            "The line_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext line_profiler\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Small\" \"/content/\"\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Large\" \"/content/\"\n",
        "!pip3 install line_profiler\n",
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "b5u30WFOTpxp"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "c6KxqY0GTQBh"
      },
      "outputs": [],
      "source": [
        "def read_from_file(file_name) :\n",
        "  csvfile = open(file = file_name, mode = 'r') # open file\n",
        "  print(type(csvfile))\n",
        "\n",
        "  csvreader = csv.reader(csvfile, skipinitialspace = True) # csv reader object\n",
        "  rows = [] # empty list\n",
        "  for row in csvreader:\n",
        "    rows.append(row)\n",
        "\n",
        "  csvfile.close() #' close file\n",
        "  return rows\n",
        "\n",
        "def write_to_file(file_name, data) :\n",
        "  file = open(file = file_name, mode = 'w') # file object\n",
        "\n",
        "  csvwriter = csv.writer(file) # csv reader object\n",
        "  for row in data:\n",
        "    csvwriter.writerow(row)\n",
        "\n",
        "  file.close() # close file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VesjogsS1upZ",
        "outputId": "6b4c73b4-a2d4-47b5-abc4-50fb7a388f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '_io.TextIOWrapper'>\n"
          ]
        }
      ],
      "source": [
        "transactions = read_from_file('Dataset.Small/GroceryStore.csv') # read transactions or database\n",
        "write_to_file('Transactions.csv', transactions) # write transactions or database into another file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiyEGq5r1Xm"
      },
      "source": [
        "**Brute-Force Approach(Frequent itemset mining)**\n",
        "\n",
        "$\n",
        "Number\\ of\\ unique\\ items\\ =\\ d\\\\\n",
        "Number\\ of\\ transactions\\ =\\ N\\\\\n",
        "Average\\ width\\ of\\ a\\ transaction=\\ w\\\\ \\\\\n",
        "Number\\ of\\ all\\ possible\\ combinations\\ =\\\n",
        "\\displaystyle\\sum_{r=1} ^{d} c_{r}\\ =\\ 2^d - 1\\\\\n",
        "Complexity\\ =\\ O(Nw2^{d})\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "nxQWILyHvSVr"
      },
      "outputs": [],
      "source": [
        "def search_database(database, itemset) :\n",
        "  \"\"\"\n",
        "    Search occurance of given itemset in the transaction database\n",
        "    Inputs\n",
        "      database : Transactions : List of lists \n",
        "      itemset : Itemset that should be searched : List\n",
        "    Outputs\n",
        "      occurance : Number of occurance : integer\n",
        "  \"\"\"\n",
        "  frequency = np.count_nonzero([all(item in transaction for item in itemset) for transaction in database])\n",
        "  return frequency\n",
        "\n",
        "def calc_candidate_sup_cnt(database, c_k, k) : \n",
        "  \"\"\"\n",
        "  Calculate support count for all candidate k-itemsets and generate dataframe\n",
        "  Inputs\n",
        "    database : Transactions : List of lists\n",
        "    c_k : Candidate k-itemsets : List of tuples\n",
        "  Output : \n",
        "    c_k_df : Candidate k-itemsets dataframe with support counts\n",
        "  \"\"\"\n",
        "  itemset_df = pd.DataFrame(columns=['key', 'item_cnt','itemset','support_cnt'])\n",
        "  for itemset in c_k: # itemset : tuple of items\n",
        "      support_count = search_database(database, itemset)\n",
        "      data_entry = {\n",
        "        \"key\" : frozenset(itemset), # immutable set\n",
        "        \"item_cnt\": k,\n",
        "        \"itemset\": itemset,\n",
        "        \"support_cnt\": support_count\n",
        "      }\n",
        "      itemset_df = itemset_df.append(data_entry, ignore_index = True)\n",
        "  return itemset_df\n",
        "\n",
        "def select_freq_itemsets(c_k, min_sup_cnt) :\n",
        "  \"\"\"\n",
        "  Select frequent k-itemsets from candidate k-itemsets(Candidate elimination)\n",
        "  Inputs\n",
        "    c_k : Candidate k-itemsets : DataFrame(itemset,support_cnt)\n",
        "    min_sup_cnt : minimum support count(hyper parameter) : integer\n",
        "  Outputs\n",
        "    f_k : Frequent k-itemsets : DataFrame(itemset,support_cnt)\n",
        "  \"\"\"\n",
        "  f_k = c_k[c_k['support_cnt'] >= min_sup_cnt] \n",
        "  return f_k\n",
        "\n",
        "def brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups):\n",
        "  \"\"\" \n",
        "  Brute Force Method \n",
        "  Inputs\n",
        "    database : Transaction database : List of lists\n",
        "    unique_items : Unique items in the database : List of items\n",
        "    min_sup_cnt : Minimum support count : intereger\n",
        "    itemset_groups : Required item groups(k values)\n",
        "  Outputs\n",
        "    all_freq_itemsets = Frequent itemsets for given groups = DataFrame\n",
        "  \"\"\"\n",
        "  all_candidate_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups: # loop number of itemset groups\n",
        "    c_k = combinations(unique_items, k) # create combinations for each itemset group\n",
        "    c_k_df = calc_candidate_sup_cnt(database, c_k, k)\n",
        "    all_candidate_df = all_candidate_df.append(c_k_df, ignore_index = True)\n",
        "\n",
        "  all_freq_itemsets = select_freq_itemsets(all_candidate_df, min_sup_cnt)\n",
        "  return all_freq_itemsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcLQWK_7lej",
        "outputId": "0ebebd7c-a6c5-4dd6-ebfd-e8b9f6c85e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of transactions : 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (3,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of large dataset : (541910, 8)\n",
            "\n",
            "\n",
            "Shape of frequent itemsets : (35, 4)\n",
            "Frequent itemsets from brute-force approach \n",
            "  item_cnt     itemset support_cnt        key\n",
            "0        1      (JAM,)           2      (JAM)\n",
            "1        1   (COFFEE,)           6   (COFFEE)\n",
            "2        1     (MILK,)           5     (MILK)\n",
            "3        1  (BISCUIT,)           6  (BISCUIT)\n",
            "4        1    (BREAD,)          11    (BREAD)\n",
            "Frequent itemsets from brute-force approach \n",
            "    item_cnt                 itemset support_cnt                     key\n",
            "64         2      (SUGER, BOURNVITA)           2      (SUGER, BOURNVITA)\n",
            "91         3     (JAM, BREAD, MAGGI)           2     (JAM, MAGGI, BREAD)\n",
            "147        3  (MILK, BISCUIT, BREAD)           2  (BISCUIT, BREAD, MILK)\n",
            "181        3   (BISCUIT, TEA, MAGGI)           2   (BISCUIT, MAGGI, TEA)\n",
            "196        3     (BREAD, TEA, MAGGI)           2     (MAGGI, BREAD, TEA)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Small dataset\"\"\"\n",
        "transaction_df = pd.read_csv('Dataset.Small/GroceryStore.csv', header=None)\n",
        "transaction_df.drop_duplicates(subset =0,\n",
        "                     keep = 'first', inplace = True)\n",
        "database = list(transaction_df[0].apply(lambda x:x.split(\",\") ))\n",
        "print(f'Size of transactions : {len(database)}')\n",
        "\n",
        "unique_items = set()\n",
        "for transaction in database:\n",
        "  for item in transaction :\n",
        "    unique_items.add(item)\n",
        "# print(unique_items)\n",
        "\n",
        "\n",
        "\"\"\" Large dataset \"\"\"\n",
        "transaction_df_large = pd.read_csv('Dataset.Large/OnlineRetail.csv', header=None)\n",
        "print(f'Size of large dataset : {transaction_df_large.shape}')\n",
        "\n",
        "min_sup_cnt = 2\n",
        "itemset_groups = [1, 2, 3, 4, 5]\n",
        "freq_itemsets = brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups)\n",
        "print(f'\\n\\nShape of frequent itemsets : {freq_itemsets.shape}')\n",
        "print(f'Frequent itemsets from brute-force approach \\n{freq_itemsets.head()}')\n",
        "print(f'Frequent itemsets from brute-force approach \\n{freq_itemsets.tail()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUYANEetVox9"
      },
      "source": [
        "**Apriori Algorithm(Frequent itemset mining)**\n",
        "\n",
        "* Superset is frequent >>> Subset is frequent (Bottom Up)\n",
        "\n",
        "* Sebset is infrequent >>> Superset is infrequent (Top down)\n",
        "\n",
        "* This algorithm has **anti-monotone** propety >>> support of itemset can not exceed support of its subset\n",
        "\n",
        "\n",
        "**Main Steps**\n",
        "\n",
        "* Candidate Generation ($F_{k-1} >> C_{k}$)\n",
        "\n",
        "* Candidate Pruning\n",
        "\n",
        "* Support Counting\n",
        "\n",
        "* Candidate Elimination ($C_{k} >> F_{k}$)\n",
        "\n",
        "**Candidate Pruning ($F_{k-1}Ã—F_{k-1}$)**\n",
        "\n",
        "$Itemset\\ is\\ not\\ frequent\\ if\\ one\\ of\\ its\\ sub\\ itemset\\ is\\ not\\ frequent$\n",
        "\n",
        "* Number of **(k-1)-size subsets** for a **k-itemset** = $C^{K}_{K-1}$ = $K$\n",
        "* Number of **already verified (k-1)-size subsets** at the candidate generation = $2$\n",
        "* Number of subsets for frequency verification stage per each **k-itemset** = $K-2$\n",
        "* Total number of subsets for frequency verification for **all k-itemsets** = $L_{k}\\times(K-2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "Ufd1lKSaVv6K"
      },
      "outputs": [],
      "source": [
        "def generate_candidates(f_prv, k):\n",
        "  \"\"\"\n",
        "  Generate candidate set for k-itemsets\n",
        "  Input : \n",
        "    f_prv : F(K-1) \n",
        "  Output : \n",
        "    c_k : List of candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  c_k = [] # candidate k-itemset\n",
        "  f_prv_pairs = combinations(f_prv, 2) # select pair of frequent (k-1)-itemsets\n",
        "  for f_prv_pair in f_prv_pairs:\n",
        "    if(len(f_prv_pair[0]) == 1) : ### K=2 ###\n",
        "      two_itemset = [*f_prv_pair[0],*f_prv_pair[1]]\n",
        "      two_itemset.sort()         \n",
        "      c_k.append(two_itemset)\n",
        "    elif(f_prv_pair[0][0:k-2] == f_prv_pair[1][0:k-2]) :### K>=2 and first (k-2) items are same in both itemsets ###\n",
        "      generated_itemset = f_prv_pair[0][0:k-2]\n",
        "      delta_items = [f_prv_pair[0][k-2],f_prv_pair[1][k-2]]\n",
        "      delta_items.sort()\n",
        "      generated_itemset = [*generated_itemset,*delta_items]\n",
        "      c_k.append(generated_itemset)\n",
        "  return c_k\n",
        "\n",
        "def prune_candidates(c_k, f_prv, k) :\n",
        "  \"\"\"\n",
        "  Prune candidate set from k-itemsets\n",
        "  Inputs\n",
        "    c_k : candidate k-itemsets : List of lists\n",
        "    f_prv : frequent (k-1)-itemsets \n",
        "  Output\n",
        "    c_k_prune : pruned candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  infrq_itemsets = []\n",
        "  ### search candidate k-itemsets to check frequency of its (k-1)-sized subsets\n",
        "  for itemset in c_k :\n",
        "    subsets = combinations(itemset, k-1)\n",
        "    for subset in subsets:\n",
        "      frequency = search_database(f_prv, subset)\n",
        "      if(frequency==0):\n",
        "        infrq_itemsets.append(itemset)\n",
        "        break\n",
        "  ### remove infrequent itemsets from candidate k-itemsets ###\n",
        "  for itemset in infrq_itemsets :\n",
        "    c_k.remove(itemset)\n",
        "\n",
        "  return c_k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnYQvEz__3Nn",
        "outputId": "e9b6b199-19a6-49c6-fa15-8803c7510fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of frequent itemsets : (35, 4)\n",
            "Frequent itemsets from apriori approach \n",
            "  item_cnt    itemset support_cnt        key\n",
            "0        1      [JAM]           2      (JAM)\n",
            "1        1   [COFFEE]           6   (COFFEE)\n",
            "2        1     [MILK]           5     (MILK)\n",
            "3        1  [BISCUIT]           6  (BISCUIT)\n",
            "4        1    [BREAD]          11    (BREAD)\n",
            "Frequent itemsets from apriori approach \n",
            "   item_cnt                 itemset support_cnt                     key\n",
            "30        2      [BOURNVITA, SUGER]           2      (SUGER, BOURNVITA)\n",
            "31        3     [BREAD, JAM, MAGGI]           2     (JAM, MAGGI, BREAD)\n",
            "32        3  [BISCUIT, BREAD, MILK]           2  (BISCUIT, BREAD, MILK)\n",
            "33        3   [BISCUIT, MAGGI, TEA]           2   (BISCUIT, MAGGI, TEA)\n",
            "34        3     [BREAD, MAGGI, TEA]           2     (MAGGI, BREAD, TEA)\n"
          ]
        }
      ],
      "source": [
        "def apriori_algorithm(min_sup_cnt) :\n",
        "  f_itemsets_store = {}\n",
        "  all_frq_itemsets_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups:\n",
        "    if(k>1) :\n",
        "      f_prv = f_itemsets_store[k-1]['itemset']\n",
        "      c_k = generate_candidates(f_prv, k) # generate candidates\n",
        "      c_k = prune_candidates(c_k, f_prv, k) # prune candidates\n",
        "      c_k = calc_candidate_sup_cnt(database, c_k, k)\n",
        "      f_k = select_freq_itemsets(c_k, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_k, ignore_index=True)\n",
        "      f_itemsets_store[k] = f_k\n",
        "    else :\n",
        "      c_1 = [[item] for item in unique_items] # candidate 1-itemset\n",
        "      c_1 = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "      f_1 = select_freq_itemsets(c_1, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_1, ignore_index=True)\n",
        "      f_itemsets_store[1] = f_1\n",
        "  return all_frq_itemsets_df, f_itemsets_store\n",
        "\n",
        "# all_itemsets_df = %lprun -f apriori_algorithm apriori_algorithm() # execute line profiler\n",
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2) # execute line profiler\n",
        "print(f'Shape of frequent itemsets : {all_frq_itemsets_df.shape}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{all_frq_itemsets_df.head()}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{all_frq_itemsets_df.tail()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDqu5cMCV8LD"
      },
      "source": [
        "**Rule Generation**\n",
        "\n",
        "Antecedent -> Consequent\n",
        "\n",
        "Ex : {X,Y} -> {P,Q}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "JglQrjGLWedO"
      },
      "outputs": [],
      "source": [
        "def generate_rules(f_k, h_m, f_itemsets_store, min_conf) :\n",
        "  \"\"\"\n",
        "  Generate rules from frequent itemsets\n",
        "  Inputs:\n",
        "    f_k : Frequent k-itemsets (X,Y,P,Q) : Tuple\n",
        "    h_m : Consequent (P,Q) : Tuple\n",
        "    f_itemsets_store : Frequent itemset store(key = k, value = support counts) : Dictionary(Key = Integer, value = DataFrame)\n",
        "  \"\"\"\n",
        "  conf = 0\n",
        "  if(len(h_m) > 0) :\n",
        "    k = len(f_k)\n",
        "    m = len(h_m[0])\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "  \n",
        "  if(k > m+1) : #### Check whether an item can be passed from antecedent to consequent >>> Terminating condition of the recursive function###\n",
        "    h_next = generate_candidates(h_m, m+1) # Generate (m+1) candidates for consequent\n",
        "    h_next = prune_candidates(h_next, h_m, m+1) # Prune (m+1) candidates\n",
        "    h_next_invalid = []\n",
        "    rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "    for h_next_i in h_next : # Iterate candidate consequents\n",
        "      consequent = h_next_i\n",
        "      antecedent = list(set(f_k) - set(h_next_i))\n",
        "      rule_sup_cnt = f_itemsets_store[k].loc[f_itemsets_store[k]['key']==frozenset(f_k)]\n",
        "      antecedent_sup_cnt = f_itemsets_store[len(antecedent)].loc[f_itemsets_store[len(antecedent)]['key']==frozenset(antecedent)]\n",
        "\n",
        "      if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "        conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "      if(conf > min_conf) : \n",
        "        data_entry = {\n",
        "          'antecedent': antecedent,\n",
        "          'consequent': consequent,\n",
        "          'confidence': conf\n",
        "        }\n",
        "        rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "      else : \n",
        "        h_next_invalid.append(h_next_i)\n",
        "\n",
        "    for h_next_invalid_i in h_next_invalid : # Remove invalid consequents \n",
        "      h_next.remove(h_next_invalid_i)\n",
        "\n",
        "    ### recursively generate rules for subgraphs \n",
        "    return generate_rules(f_k, h_next, f_itemsets_store, min_conf).append(rule_df, ignore_index=True)\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l5XsaZHP9m0",
        "outputId": "6b096fd4-1d46-45fa-bf51-f6993cff76ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(35, 4)\n",
            "          antecedent      consequent  confidence\n",
            "0              [JAM]         [BREAD]    1.000000\n",
            "1              [JAM]         [MAGGI]    1.000000\n",
            "2             [COCK]        [COFFEE]    1.000000\n",
            "3            [SUGER]        [COFFEE]    0.600000\n",
            "4           [COFFEE]         [SUGER]    0.500000\n",
            "5       [CORNFLAKES]        [COFFEE]    0.600000\n",
            "6           [COFFEE]    [CORNFLAKES]    0.500000\n",
            "7             [MILK]         [BREAD]    0.800000\n",
            "8          [BISCUIT]         [BREAD]    0.666667\n",
            "9              [TEA]         [BREAD]    0.500000\n",
            "10           [MAGGI]         [BREAD]    0.600000\n",
            "11           [SUGER]         [BREAD]    0.600000\n",
            "12       [BOURNVITA]         [BREAD]    0.666667\n",
            "13             [TEA]         [MAGGI]    0.666667\n",
            "14           [MAGGI]           [TEA]    0.800000\n",
            "15       [BOURNVITA]         [SUGER]    0.666667\n",
            "16      [JAM, MAGGI]         [BREAD]    1.000000\n",
            "17    [BREAD, MAGGI]           [JAM]    0.666667\n",
            "18      [JAM, BREAD]         [MAGGI]    1.000000\n",
            "19             [JAM]  [BREAD, MAGGI]    1.000000\n",
            "20     [BREAD, MILK]       [BISCUIT]    0.500000\n",
            "21   [BISCUIT, MILK]         [BREAD]    1.000000\n",
            "22  [BISCUIT, BREAD]          [MILK]    0.500000\n",
            "23      [MAGGI, TEA]       [BISCUIT]    0.500000\n",
            "24    [BISCUIT, TEA]         [MAGGI]    1.000000\n",
            "25  [BISCUIT, MAGGI]           [TEA]    1.000000\n",
            "26      [MAGGI, TEA]         [BREAD]    0.500000\n",
            "27      [BREAD, TEA]         [MAGGI]    0.666667\n",
            "28    [BREAD, MAGGI]           [TEA]    0.666667\n",
            "\n",
            "\n",
            "Shape of association rules : (29, 3)\n",
            "Association rules(Head) \n",
            "  antecedent consequent  confidence\n",
            "0      [JAM]    [BREAD]         1.0\n",
            "1      [JAM]    [MAGGI]         1.0\n",
            "2     [COCK]   [COFFEE]         1.0\n",
            "3    [SUGER]   [COFFEE]         0.6\n",
            "4   [COFFEE]    [SUGER]         0.5\n",
            "Association rules(Tail) \n",
            "          antecedent consequent  confidence\n",
            "24    [BISCUIT, TEA]    [MAGGI]    1.000000\n",
            "25  [BISCUIT, MAGGI]      [TEA]    1.000000\n",
            "26      [MAGGI, TEA]    [BREAD]    0.500000\n",
            "27      [BREAD, TEA]    [MAGGI]    0.666667\n",
            "28    [BREAD, MAGGI]      [TEA]    0.666667\n"
          ]
        }
      ],
      "source": [
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2)\n",
        "min_conf = 0.5\n",
        "print(all_frq_itemsets_df.shape)\n",
        "\n",
        "rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "for f_k in all_frq_itemsets_df['itemset'] :\n",
        "  h_1_invalid = []\n",
        "  for item in f_k :\n",
        "    if len(f_k)==1 :\n",
        "      continue\n",
        "    consequent = []\n",
        "    consequent.append(item)\n",
        "    antecedent = list(set(f_k)-set(consequent))\n",
        "\n",
        "    rule_sup_cnt = f_itemsets_store[len(f_k)].loc[f_itemsets_store[len(f_k)]['key']==frozenset(f_k)]\n",
        "    antecedent_sup_cnt = f_itemsets_store[len(antecedent)].loc[f_itemsets_store[len(antecedent)]['key']==frozenset(antecedent)]\n",
        "\n",
        "    if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "      conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "    if(conf >= min_conf) : \n",
        "      data_entry = {\n",
        "        'antecedent': antecedent,\n",
        "        'consequent': consequent,\n",
        "        'confidence': conf\n",
        "      }\n",
        "      rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "    else : \n",
        "      h_1_invalid.append(consequent)\n",
        "\n",
        "  h_1 = [[item] for item in f_k] # create consequents for single items\n",
        "  if len(h_1_invalid)>0 : # remove low confidence items from consequents\n",
        "    for h_1_invalid_i in h_1_invalid : # Remove invalid consequents \n",
        "      h_1.remove(h_1_invalid_i)\n",
        "  if len(h_1)==0 :\n",
        "    continue\n",
        "    \n",
        "  rule_df = rule_df.append(generate_rules(f_k, h_1, f_itemsets_store, min_conf), ignore_index=True)\n",
        "\n",
        "print(rule_df)\n",
        "print(f'\\n\\nShape of association rules : {rule_df.shape}')\n",
        "print(f'Association rules(Head) \\n{rule_df.head()}')\n",
        "print(f'Association rules(Tail) \\n{rule_df.tail()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3kK-iwgTmIH"
      },
      "source": [
        "**FP-Growth Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "dVp5lYHcX7sS"
      },
      "outputs": [],
      "source": [
        "class ItemData :\n",
        "  def __init__(self, item, sup_count):\n",
        "    self.item = item\n",
        "    self.sup_count = sup_count\n",
        "    self.con_sup_cnt = 0\n",
        "\n",
        "  def __str__(self):\n",
        "     return str({\n",
        "         \"item\" : self.item,\n",
        "         \"sup_count\" : self.sup_count\n",
        "     })\n",
        "\n",
        "  def __cmp__(self,other):\n",
        "    return self.count<other.count\n",
        "\n",
        "class TreeNode :\n",
        "  def __init__(self, data):\n",
        "      self.parent = None\n",
        "      self.children = {}\n",
        "      self.next = None\n",
        "      self.data = data\n",
        "\n",
        "class ReferenceNode : \n",
        "  def __init__(self) :\n",
        "    self.head = None\n",
        "    self.tail = None\n",
        "    self.sup_count = 0\n",
        "    self.con_sup_cnt = 0\n",
        "    self.item = None\n",
        "  \n",
        "  def __str__(self):\n",
        "     return str({\n",
        "         \"item\" : self.item,\n",
        "         \"sup_count\" : self.sup_count,\n",
        "         \"con_sup_count\" : self.con_sup_cnt\n",
        "     })\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ6oD_scdMKv",
        "outputId": "0d2c3e4b-1807-4a9e-a2b9-049ce523aa1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infrequent Row : Empty DataFrame\n",
            "Columns: [key, item_cnt, itemset, support_cnt]\n",
            "Index: []\n",
            "\n",
            "Infrequent Items : set()\n",
            "\n",
            "Frequent single items :              key item_cnt       itemset support_cnt\n",
            "4        (BREAD)        1       [BREAD]          11\n",
            "1       (COFFEE)        1      [COFFEE]           6\n",
            "3      (BISCUIT)        1     [BISCUIT]           6\n",
            "5          (TEA)        1         [TEA]           6\n",
            "2         (MILK)        1        [MILK]           5\n",
            "6        (MAGGI)        1       [MAGGI]           5\n",
            "8        (SUGER)        1       [SUGER]           5\n",
            "9   (CORNFLAKES)        1  [CORNFLAKES]           5\n",
            "10   (BOURNVITA)        1   [BOURNVITA]           3\n",
            "0          (JAM)        1         [JAM]           2\n",
            "7         (COCK)        1        [COCK]           2\n",
            "\n",
            "\n",
            "Length of pruned database : 17\n",
            "Support Count of BREAD : 11\n",
            "{'item': 'BREAD', 'sup_count': 11}\n",
            "\n",
            "Support Count of BISCUIT : 6\n",
            "{'item': 'BISCUIT', 'sup_count': 4}\n",
            "{'item': 'BISCUIT', 'sup_count': 2}\n",
            "\n",
            "Support Count of MILK : 5\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "{'item': 'MILK', 'sup_count': 1}\n",
            "\n",
            "Support Count of CORNFLAKES : 5\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "{'item': 'CORNFLAKES', 'sup_count': 1}\n",
            "\n",
            "Support Count of TEA : 6\n",
            "{'item': 'TEA', 'sup_count': 2}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "{'item': 'TEA', 'sup_count': 1}\n",
            "\n",
            "Support Count of BOURNVITA : 3\n",
            "{'item': 'BOURNVITA', 'sup_count': 1}\n",
            "{'item': 'BOURNVITA', 'sup_count': 1}\n",
            "{'item': 'BOURNVITA', 'sup_count': 1}\n",
            "\n",
            "Support Count of MAGGI : 5\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "{'item': 'MAGGI', 'sup_count': 1}\n",
            "\n",
            "Support Count of JAM : 2\n",
            "{'item': 'JAM', 'sup_count': 1}\n",
            "{'item': 'JAM', 'sup_count': 1}\n",
            "\n",
            "Support Count of COFFEE : 6\n",
            "{'item': 'COFFEE', 'sup_count': 1}\n",
            "{'item': 'COFFEE', 'sup_count': 3}\n",
            "{'item': 'COFFEE', 'sup_count': 2}\n",
            "\n",
            "Support Count of COCK : 2\n",
            "{'item': 'COCK', 'sup_count': 1}\n",
            "{'item': 'COCK', 'sup_count': 1}\n",
            "\n",
            "Support Count of SUGER : 5\n",
            "{'item': 'SUGER', 'sup_count': 2}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "{'item': 'SUGER', 'sup_count': 1}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_fp_tree(min_sup_cnt):\n",
        "\n",
        "  ############################# Part (1) ##################################################\n",
        "  c_1 = [[item] for item in unique_items]\n",
        "  itemset_df = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "\n",
        "  infrequent_rows = itemset_df[ itemset_df['support_cnt'] < min_sup_cnt ] # filter items with lower confidence and retrieve indices of them\n",
        "  print(f'Infrequent Row : {infrequent_rows}\\n')\n",
        "  infrequent_items = set(row['itemset'][0] for index,row in infrequent_rows.iterrows()) ################################## TODO : Need to review ##########################################\n",
        "  print( f'Infrequent Items : {infrequent_items}\\n')\n",
        "  \n",
        "  #### Drop infrequent items ####\n",
        "  itemset_df.drop(infrequent_rows.index , inplace=True)\n",
        "  #### Sort according to support count ####\n",
        "  itemset_df = itemset_df.sort_values(by = ['support_cnt'], ascending=False)\n",
        "  print(f'Frequent single items : {itemset_df}\\n\\n')\n",
        " \n",
        "  #### Remove infrequent items from every transaction and sort by support count in descending order ####\n",
        "  database_node_model = []\n",
        "  for transaction in database:\n",
        "    transaction_prune = set(transaction)-infrequent_items # remove infrequent items from every transaction\n",
        "\n",
        "    transaction_node_model = []\n",
        "    for item in transaction_prune : \n",
        "      sup_cnt = itemset_df[itemset_df['key']==frozenset([item])]['support_cnt'].values[0]\n",
        "      #### Create item object for each item in every transaction ###\n",
        "      item_data = ItemData(item, sup_cnt)\n",
        "      transaction_node_model.append(item_data)\n",
        "    \n",
        "    #### Sort item nodes in transaction accoring to support count (Descending order) ####\n",
        "    transaction_node_model.sort(key = lambda c: c.sup_count, reverse=True) \n",
        "    database_node_model.append(transaction_node_model)\n",
        "  print(f'Length of pruned database : {len(database_node_model)}')\n",
        "\n",
        "  ############################# Part (2) ##################################################\n",
        "  \n",
        "  #### Root Node : Null ####\n",
        "  root_data = ItemData(item = None, sup_count = None)\n",
        "  root_node = TreeNode(root_data) \n",
        "  #### Item chain maintain dictionary ####\n",
        "  reference_keeper = {}\n",
        "  \n",
        "  ### Construct the model ####\n",
        "  for transaction in database_node_model :\n",
        "    current_root_locator = root_node\n",
        "    for search_data in transaction:\n",
        "      current_root_locator = traverse(current_root_locator, search_data, reference_keeper)\n",
        "\n",
        "  return root_node, reference_keeper\n",
        "\n",
        "\n",
        "\n",
        "def traverse(root_node, search_data, reference_keeper):\n",
        "  current_root_locator = root_node\n",
        "  if search_data.item not in current_root_locator.children:\n",
        "    search_data.sup_count = 1\n",
        "    search_node = TreeNode(search_data)\n",
        "    search_node.parent = current_root_locator\n",
        "    current_root_locator.children[search_data.item] = search_node\n",
        "    current_root_locator = search_node\n",
        "\n",
        "    if search_data.item not in reference_keeper :\n",
        "      ##### Create reference for the new item ####\n",
        "      reference_node = ReferenceNode()\n",
        "      reference_node.head = search_node\n",
        "      reference_node.tail = search_node\n",
        "      reference_node.sup_count = search_data.sup_count\n",
        "      reference_node.item = search_data.item\n",
        "      reference_keeper[search_data.item] = reference_node\n",
        "    else :\n",
        "      reference_node = reference_keeper[search_data.item]\n",
        "      tail_node = reference_node.tail\n",
        "      tail_node.next = search_node\n",
        "      reference_node.tail = search_node\n",
        "      reference_node.sup_count += search_data.sup_count\n",
        "      reference_keeper[search_data.item] = reference_node\n",
        "  else :\n",
        "    current_root_locator = current_root_locator.children[search_data.item]\n",
        "    current_root_locator.data.sup_count += 1\n",
        "\n",
        "    ##### Update Reference  ####\n",
        "    reference_node = reference_keeper[search_data.item]\n",
        "    reference_node.sup_count += 1\n",
        "  return current_root_locator\n",
        "  \n",
        "def print_tree(tree_node) : \n",
        "  if(tree_node == None ) :\n",
        "    return\n",
        "  print(f'{tree_node.data}')\n",
        "  children = tree_node.children\n",
        "  if(len(children)==0) :\n",
        "    return\n",
        "  for child in children.values():\n",
        "    print_tree(child)\n",
        "\n",
        "def generate_fp_itemsets(root_node, reference_keeper, conditional_itemset, min_sup_cnt):\n",
        "\n",
        "  ### sort references in descending order of support count ####\n",
        "  reference_keeper = dict(sorted(reference_keeper.items(), key=lambda item: item[1].sup_count))\n",
        "\n",
        "\n",
        "  for (item,reference) in reference_keeper.items() :\n",
        "    if(len(conditional_itemset)>0 and reference.con_sup_cnt < min_sup_cnt) :\n",
        "      continue\n",
        "   \n",
        "    ##################### Update conditional support counts from bottom to top ###########################\n",
        "    con_reference_keeper = {}\n",
        "\n",
        "    current_x_ref = reference.head\n",
        "\n",
        "    conditional_itemset = []\n",
        "    if len(conditional_itemset)==0:\n",
        "      conditional_itemset = [current_x_ref.data.item]\n",
        "    else:\n",
        "      conditional_itemset = [current_x_ref.data.item, conditional_itemset[0]]\n",
        "    \n",
        "    print(conditional_itemset)\n",
        "\n",
        "    while(current_x_ref != None) : \n",
        "      current_x_ref.data.con_sup_cnt = 0\n",
        "      current_y_ref = current_x_ref.parent\n",
        "\n",
        "      #### Update conditional support counts through prefix paths from bottom to top ####\n",
        "      while(current_y_ref.data.item != None): #### vertical movement ####\n",
        "\n",
        "        if current_y_ref.data.item in con_reference_keeper :\n",
        "          current_pointer = con_reference_keeper.get(current_y_ref.data.item)\n",
        "          current_pointer.con_sup_cnt += 1\n",
        "        else :\n",
        "          current_pointer = ReferenceNode()\n",
        "          current_pointer.head = current_y_ref\n",
        "          current_pointer.tail = current_y_ref\n",
        "          current_pointer.sup_count = current_y_ref.data.sup_count\n",
        "          current_pointer.item = current_y_ref.data.item\n",
        "          current_pointer.con_sup_cnt = 1\n",
        "          con_reference_keeper[current_y_ref.data.item] = current_pointer\n",
        "\n",
        "        current_y_ref.data.con_sup_cnt += 1\n",
        "        current_y_ref = current_y_ref.parent\n",
        "\n",
        "      current_x_ref = current_x_ref.next   #### Horizontal movement ####\n",
        "      \n",
        "      ###################### remove low support count nodes from the tree #################################\n",
        "      # for (key, reference) in con_reference_keeper.items():\n",
        "      #   if(reference.con_sup_cnt<min_sup_cnt):\n",
        "      #     current_pointer = reference.head \n",
        "      #     current_parent = current_pointer.parent\n",
        "      #     for child_node in current_pointer.children:\n",
        "      #       child_node.parent = current_parent\n",
        "      \n",
        "    \n",
        "    print(generate_fp_itemsets(root_node, con_reference_keeper, conditional_itemset, min_sup_cnt))\n",
        "      \n",
        "      \n",
        "      \n",
        "########################################### Testing ###################################################################\n",
        "(root_node, reference_keeper) = generate_fp_tree(min_sup_cnt = 2)\n",
        "#### Print reference chain for each frequent items ####\n",
        "for (key,value) in reference_keeper.items() :\n",
        "  print(f'Support Count of {value.head.data.item} : {value.sup_count}')\n",
        "  temp = value.head\n",
        "  while(temp != None) :\n",
        "    print(temp.data)\n",
        "    temp = temp.next\n",
        "  print()\n",
        "\n",
        "#### Generate itemsets using FP tree ####\n",
        "# generate_fp_itemsets(root_node, reference_keeper, [], 2)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhtVLsVvWSJ9"
      },
      "source": [
        "References\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/08/python-tutorial-working-with-csv-file-for-data-science/\n",
        "\n",
        "https://towardsdatascience.com/magic-commands-for-profiling-in-jupyter-notebook-d2ef00e29a63\n",
        "\n",
        "http://www.btechsmartclass.com/data_structures/tree-terminology.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asktm-VbnROI",
        "outputId": "5acb6815-9d1d-4816-d6ac-48e618245db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of frequent itemsets : (35, 2)\n",
            "Frequent itemsets from apriori \n",
            "    support     itemsets\n",
            "0  0.352941    (BISCUIT)\n",
            "1  0.176471  (BOURNVITA)\n",
            "2  0.647059      (BREAD)\n",
            "3  0.117647       (COCK)\n",
            "4  0.352941     (COFFEE)\n",
            "Frequent itemsets from apriori approach \n",
            "     support                itemsets\n",
            "30  0.235294            (MAGGI, TEA)\n",
            "31  0.117647  (BISCUIT, BREAD, MILK)\n",
            "32  0.117647   (BISCUIT, MAGGI, TEA)\n",
            "33  0.117647     (JAM, MAGGI, BREAD)\n",
            "34  0.117647     (MAGGI, BREAD, TEA)\n",
            "         antecedents     consequents  antecedent support  consequent support  \\\n",
            "0          (BISCUIT)         (BREAD)            0.352941            0.647059   \n",
            "1        (BOURNVITA)         (BREAD)            0.176471            0.647059   \n",
            "2        (BOURNVITA)         (SUGER)            0.176471            0.294118   \n",
            "3              (JAM)         (BREAD)            0.117647            0.647059   \n",
            "4            (MAGGI)         (BREAD)            0.294118            0.647059   \n",
            "5             (MILK)         (BREAD)            0.294118            0.647059   \n",
            "6            (SUGER)         (BREAD)            0.294118            0.647059   \n",
            "7              (TEA)         (BREAD)            0.352941            0.647059   \n",
            "8             (COCK)        (COFFEE)            0.117647            0.352941   \n",
            "9       (CORNFLAKES)        (COFFEE)            0.294118            0.352941   \n",
            "10          (COFFEE)    (CORNFLAKES)            0.352941            0.294118   \n",
            "11           (SUGER)        (COFFEE)            0.294118            0.352941   \n",
            "12          (COFFEE)         (SUGER)            0.352941            0.294118   \n",
            "13             (JAM)         (MAGGI)            0.117647            0.294118   \n",
            "14           (MAGGI)           (TEA)            0.294118            0.352941   \n",
            "15             (TEA)         (MAGGI)            0.352941            0.294118   \n",
            "16  (BISCUIT, BREAD)          (MILK)            0.235294            0.294118   \n",
            "17   (BISCUIT, MILK)         (BREAD)            0.117647            0.647059   \n",
            "18     (BREAD, MILK)       (BISCUIT)            0.235294            0.352941   \n",
            "19  (BISCUIT, MAGGI)           (TEA)            0.117647            0.352941   \n",
            "20    (BISCUIT, TEA)         (MAGGI)            0.117647            0.294118   \n",
            "21      (MAGGI, TEA)       (BISCUIT)            0.235294            0.352941   \n",
            "22      (JAM, MAGGI)         (BREAD)            0.117647            0.647059   \n",
            "23      (JAM, BREAD)         (MAGGI)            0.117647            0.294118   \n",
            "24    (BREAD, MAGGI)           (JAM)            0.176471            0.117647   \n",
            "25             (JAM)  (BREAD, MAGGI)            0.117647            0.176471   \n",
            "26    (BREAD, MAGGI)           (TEA)            0.176471            0.352941   \n",
            "27      (MAGGI, TEA)         (BREAD)            0.235294            0.647059   \n",
            "28      (BREAD, TEA)         (MAGGI)            0.176471            0.294118   \n",
            "\n",
            "     support  confidence      lift  leverage  conviction  \n",
            "0   0.235294    0.666667  1.030303  0.006920    1.058824  \n",
            "1   0.117647    0.666667  1.030303  0.003460    1.058824  \n",
            "2   0.117647    0.666667  2.266667  0.065744    2.117647  \n",
            "3   0.117647    1.000000  1.545455  0.041522         inf  \n",
            "4   0.176471    0.600000  0.927273 -0.013841    0.882353  \n",
            "5   0.235294    0.800000  1.236364  0.044983    1.764706  \n",
            "6   0.176471    0.600000  0.927273 -0.013841    0.882353  \n",
            "7   0.176471    0.500000  0.772727 -0.051903    0.705882  \n",
            "8   0.117647    1.000000  2.833333  0.076125         inf  \n",
            "9   0.176471    0.600000  1.700000  0.072664    1.617647  \n",
            "10  0.176471    0.500000  1.700000  0.072664    1.411765  \n",
            "11  0.176471    0.600000  1.700000  0.072664    1.617647  \n",
            "12  0.176471    0.500000  1.700000  0.072664    1.411765  \n",
            "13  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "14  0.235294    0.800000  2.266667  0.131488    3.235294  \n",
            "15  0.235294    0.666667  2.266667  0.131488    2.117647  \n",
            "16  0.117647    0.500000  1.700000  0.048443    1.411765  \n",
            "17  0.117647    1.000000  1.545455  0.041522         inf  \n",
            "18  0.117647    0.500000  1.416667  0.034602    1.294118  \n",
            "19  0.117647    1.000000  2.833333  0.076125         inf  \n",
            "20  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "21  0.117647    0.500000  1.416667  0.034602    1.294118  \n",
            "22  0.117647    1.000000  1.545455  0.041522         inf  \n",
            "23  0.117647    1.000000  3.400000  0.083045         inf  \n",
            "24  0.117647    0.666667  5.666667  0.096886    2.647059  \n",
            "25  0.117647    1.000000  5.666667  0.096886         inf  \n",
            "26  0.117647    0.666667  1.888889  0.055363    1.941176  \n",
            "27  0.117647    0.500000  0.772727 -0.034602    0.705882  \n",
            "28  0.117647    0.666667  2.266667  0.065744    2.117647  \n",
            "\n",
            "\n",
            "Shape of association rules : (29, 9)\n",
            "Association rules(Head) \n",
            "   antecedents consequents  antecedent support  consequent support   support  \\\n",
            "0    (BISCUIT)     (BREAD)            0.352941            0.647059  0.235294   \n",
            "1  (BOURNVITA)     (BREAD)            0.176471            0.647059  0.117647   \n",
            "2  (BOURNVITA)     (SUGER)            0.176471            0.294118  0.117647   \n",
            "3        (JAM)     (BREAD)            0.117647            0.647059  0.117647   \n",
            "4      (MAGGI)     (BREAD)            0.294118            0.647059  0.176471   \n",
            "\n",
            "   confidence      lift  leverage  conviction  \n",
            "0    0.666667  1.030303  0.006920    1.058824  \n",
            "1    0.666667  1.030303  0.003460    1.058824  \n",
            "2    0.666667  2.266667  0.065744    2.117647  \n",
            "3    1.000000  1.545455  0.041522         inf  \n",
            "4    0.600000  0.927273 -0.013841    0.882353  \n",
            "Association rules(Tail) \n",
            "       antecedents     consequents  antecedent support  consequent support  \\\n",
            "24  (BREAD, MAGGI)           (JAM)            0.176471            0.117647   \n",
            "25           (JAM)  (BREAD, MAGGI)            0.117647            0.176471   \n",
            "26  (BREAD, MAGGI)           (TEA)            0.176471            0.352941   \n",
            "27    (MAGGI, TEA)         (BREAD)            0.235294            0.647059   \n",
            "28    (BREAD, TEA)         (MAGGI)            0.176471            0.294118   \n",
            "\n",
            "     support  confidence      lift  leverage  conviction  \n",
            "24  0.117647    0.666667  5.666667  0.096886    2.647059  \n",
            "25  0.117647    1.000000  5.666667  0.096886         inf  \n",
            "26  0.117647    0.666667  1.888889  0.055363    1.941176  \n",
            "27  0.117647    0.500000  0.772727 -0.034602    0.705882  \n",
            "28  0.117647    0.666667  2.266667  0.065744    2.117647  \n"
          ]
        }
      ],
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "#Let's transform the list, with one-hot encoding\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "a = TransactionEncoder()\n",
        "a_data = a.fit(database).transform(database)\n",
        "df = pd.DataFrame(a_data,columns=a.columns_)\n",
        "df = df.replace(False,0)\n",
        "df\n",
        "\n",
        "\n",
        "df = apriori(df, min_support = 0.1, use_colnames = True)\n",
        "print(f'Shape of frequent itemsets : {df.shape}')\n",
        "print(f'Frequent itemsets from apriori \\n{df.head()}')\n",
        "print(f'Frequent itemsets from apriori approach \\n{df.tail()}')\n",
        "\n",
        "df_ar = association_rules(df, metric = \"confidence\", min_threshold = 0.5)\n",
        "print(df_ar)\n",
        "print(f'\\n\\nShape of association rules : {df_ar.shape}')\n",
        "print(f'Association rules(Head) \\n{df_ar.head()}')\n",
        "print(f'Association rules(Tail) \\n{df_ar.tail()}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Associated Rule Mining.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSb4p5o26WQ1d7o5V+lk+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
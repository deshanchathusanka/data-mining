{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Associated Rule Mining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH78x0wdQxPJ",
        "outputId": "5a97722e-6c0c-4c97-fb4d-f71e262c216a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/Dataset.Small': File exists\n",
            "ln: failed to create symbolic link '/content/Dataset.Large': File exists\n",
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.7/dist-packages (3.5.1)\n",
            "The line_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext line_profiler\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Small\" \"/content/\"\n",
        "!ln -s \"/content/drive/My Drive/Academic/CSCM35 - Big Data & Data Mining/coursework 2/Dataset.Large\" \"/content/\"\n",
        "!pip3 install line_profiler\n",
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "b5u30WFOTpxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_from_file(file_name) :\n",
        "  csvfile = open(file = file_name, mode = 'r') # open file\n",
        "  print(type(csvfile))\n",
        "\n",
        "  csvreader = csv.reader(csvfile, skipinitialspace = True) # csv reader object\n",
        "  rows = [] # empty list\n",
        "  for row in csvreader:\n",
        "    rows.append(row)\n",
        "\n",
        "  csvfile.close() #' close file\n",
        "  return rows\n",
        "\n",
        "def write_to_file(file_name, data) :\n",
        "  file = open(file = file_name, mode = 'w') # file object\n",
        "\n",
        "  csvwriter = csv.writer(file) # csv reader object\n",
        "  for row in data:\n",
        "    csvwriter.writerow(row)\n",
        "\n",
        "  file.close() # close file"
      ],
      "metadata": {
        "id": "c6KxqY0GTQBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = read_from_file('Dataset.Small/GroceryStore.csv') # read transactions or database\n",
        "write_to_file('Transactions.csv', transactions) # write transactions or database into another file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VesjogsS1upZ",
        "outputId": "861c589c-8ff5-4da7-84ff-2c14ae9c9421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '_io.TextIOWrapper'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Brute-Force Approach(Frequent itemset mining)**\n",
        "\n",
        "$\n",
        "Number\\ of\\ unique\\ items\\ =\\ d\\\\\n",
        "Number\\ of\\ transactions\\ =\\ N\\\\\n",
        "Average\\ width\\ of\\ a\\ transaction=\\ w\\\\ \\\\\n",
        "Number\\ of\\ all\\ possible\\ combinations\\ =\\\n",
        "\\displaystyle\\sum_{r=1} ^{d} c_{r}\\ =\\ 2^d - 1\\\\\n",
        "Complexity\\ =\\ O(Nw2^{d})\n",
        "$"
      ],
      "metadata": {
        "id": "hiiyEGq5r1Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_database(database, itemset) :\n",
        "  \"\"\"\n",
        "    Search occurance of given itemset in the transaction database\n",
        "    Inputs\n",
        "      database : Transactions : List of lists \n",
        "      itemset : Itemset that should be searched : List\n",
        "    Outputs\n",
        "      occurance : Number of occurance : integer\n",
        "  \"\"\"\n",
        "  frequency = np.count_nonzero([all(item in transaction for item in itemset) for transaction in database])\n",
        "  return frequency\n",
        "\n",
        "def calc_candidate_sup_cnt(database, c_k, k) : \n",
        "  \"\"\"\n",
        "  Calculate support count for all candidate k-itemsets and generate dataframe\n",
        "  Inputs\n",
        "    database : Transactions : List of lists\n",
        "    c_k : Candidate k-itemsets : List of tuples\n",
        "  Output : \n",
        "    c_k_df : Candidate k-itemsets dataframe with support counts\n",
        "  \"\"\"\n",
        "  itemset_df = pd.DataFrame(columns=['key', 'item_cnt','itemset','support_cnt'])\n",
        "  for itemset in c_k: # itemset : tuple of items\n",
        "      support_count = search_database(database, itemset)\n",
        "      data_entry = {\n",
        "        \"key\" : frozenset(itemset), # immutable set\n",
        "        \"item_cnt\": k,\n",
        "        \"itemset\": itemset,\n",
        "        \"support_cnt\": support_count\n",
        "      }\n",
        "      itemset_df = itemset_df.append(data_entry, ignore_index = True)\n",
        "  return itemset_df\n",
        "\n",
        "def select_freq_itemsets(c_k, min_sup_cnt) :\n",
        "  \"\"\"\n",
        "  Select frequent k-itemsets from candidate k-itemsets(Candidate elimination)\n",
        "  Inputs\n",
        "    c_k : Candidate k-itemsets : DataFrame(itemset,support_cnt)\n",
        "    min_sup_cnt : minimum support count(hyper parameter) : integer\n",
        "  Outputs\n",
        "    f_k : Frequent k-itemsets : DataFrame(itemset,support_cnt)\n",
        "  \"\"\"\n",
        "  f_k = c_k[c_k['support_cnt'] >= min_sup_cnt] \n",
        "  return f_k\n",
        "\n",
        "def brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups):\n",
        "  \"\"\" \n",
        "  Brute Force Method \n",
        "  Inputs\n",
        "    database : Transaction database : List of lists\n",
        "    unique_items : Unique items in the database : List of items\n",
        "    min_sup_cnt : Minimum support count : intereger\n",
        "    itemset_groups : Required item groups(k values)\n",
        "  Outputs\n",
        "    all_freq_itemsets = Frequent itemsets for given groups = DataFrame\n",
        "  \"\"\"\n",
        "  all_candidate_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups: # loop number of itemset groups\n",
        "    c_k = combinations(unique_items, k) # create combinations for each itemset group\n",
        "    c_k_df = calc_candidate_sup_cnt(database, c_k, k)\n",
        "    all_candidate_df = all_candidate_df.append(c_k_df, ignore_index = True)\n",
        "\n",
        "  all_freq_itemsets = select_freq_itemsets(all_candidate_df, min_sup_cnt)\n",
        "  return all_freq_itemsets"
      ],
      "metadata": {
        "id": "nxQWILyHvSVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Small dataset\"\"\"\n",
        "transactions = read_from_file('Dataset.Small/GroceryStore.csv') \n",
        "database = [] # transactions or database\n",
        "for transaction in transactions:\n",
        "  tr_str = ''.join(transaction)\n",
        "  database.append(tr_str.split(','))\n",
        "unique_items = read_from_file('Dataset.Small/Items.txt')[0] # unique items\n",
        "\n",
        "\"\"\" Large dataset \"\"\"\n",
        "# transactions = read_from_file('Dataset.Large/OnlineRetail.csv') \n",
        "# database = [] # transactions or database\n",
        "# for transaction in transactions:\n",
        "#   tr_str = ''.join(transaction)\n",
        "#   database.append(tr_str.split(','))\n",
        "# unique_items = read_from_file('Dataset.Small/Items.txt')[0] # unique items\n",
        "# print(database)\n",
        "\n",
        "min_sup_cnt = 2\n",
        "itemset_groups = [1, 2, 3, 4, 5]\n",
        "freq_itemsets = brute_force_approach(database, unique_items, min_sup_cnt, itemset_groups)\n",
        "print(freq_itemsets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcLQWK_7lej",
        "outputId": "50bd73c2-3198-4528-e928-06d6f36ade84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '_io.TextIOWrapper'>\n",
            "<class '_io.TextIOWrapper'>\n",
            "(33, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apriori Algorithm(Frequent itemset mining)**\n",
        "\n",
        "* Superset is frequent >>> Subset is frequent (Bottom Up)\n",
        "\n",
        "* Sebset is infrequent >>> Superset is infrequent (Top down)\n",
        "\n",
        "* This algorithm has **anti-monotone** propety >>> support of itemset can not exceed support of its subset\n",
        "\n",
        "\n",
        "**Main Steps**\n",
        "\n",
        "* Candidate Generation ($F_{k-1} >> C_{k}$)\n",
        "\n",
        "* Candidate Pruning\n",
        "\n",
        "* Support Counting\n",
        "\n",
        "* Candidate Elimination ($C_{k} >> F_{k}$)\n",
        "\n",
        "**Candidate Pruning ($F_{k-1}×F_{k-1}$)**\n",
        "\n",
        "$Itemset\\ is\\ not\\ frequent\\ if\\ one\\ of\\ its\\ sub\\ itemset\\ is\\ not\\ frequent$\n",
        "\n",
        "* Number of **(k-1)-size subsets** for a **k-itemset** = $C^{K}_{K-1}$ = $K$\n",
        "* Number of **already verified (k-1)-size subsets** at the candidate generation = $2$\n",
        "* Number of subsets for frequency verification stage per each **k-itemset** = $K-2$\n",
        "* Total number of subsets for frequency verification for **all k-itemsets** = $L_{k}\\times(K-2)$"
      ],
      "metadata": {
        "id": "lUYANEetVox9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(f_prv, k):\n",
        "  \"\"\"\n",
        "  Generate candidate set for k-itemsets\n",
        "  Input : \n",
        "    f_prv : F(K-1) \n",
        "  Output : \n",
        "    c_k : List of candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  c_k = [] # candidate k-itemset\n",
        "  f_prv_pairs = combinations(f_prv, 2) # select pair of frequent (k-1)-itemsets\n",
        "  for f_prv_pair in f_prv_pairs:\n",
        "    if(len(f_prv_pair[0]) == 1) : ### K=2 ###\n",
        "      two_itemset = [*f_prv_pair[0],*f_prv_pair[1]]\n",
        "      two_itemset.sort()         \n",
        "      c_k.append(two_itemset)\n",
        "    elif(f_prv_pair[0][0:k-2] == f_prv_pair[1][0:k-2]) :### K>=2 and first (k-2) items are same in both itemsets ###\n",
        "      generated_itemset = f_prv_pair[0][0:k-2]\n",
        "      delta_items = [f_prv_pair[0][k-2],f_prv_pair[1][k-2]]\n",
        "      delta_items.sort()\n",
        "      generated_itemset = [*generated_itemset,*delta_items]\n",
        "      c_k.append(generated_itemset)\n",
        "  return c_k\n",
        "\n",
        "def prune_candidates(c_k, f_prv, k) :\n",
        "  \"\"\"\n",
        "  Prune candidate set from k-itemsets\n",
        "  Inputs\n",
        "    c_k : candidate k-itemsets : List of lists\n",
        "    f_prv : frequent (k-1)-itemsets \n",
        "  Output\n",
        "    c_k_prune : pruned candidate k-itemsets : List of lists\n",
        "  \"\"\"\n",
        "  infrq_itemsets = []\n",
        "  ### search candidate k-itemsets to check frequency of its (k-1)-sized subsets\n",
        "  for itemset in c_k :\n",
        "    subsets = combinations(itemset, k-1)\n",
        "    for subset in subsets:\n",
        "      frequency = search_database(f_prv, subset)\n",
        "      if(frequency==0):\n",
        "        infrq_itemsets.append(itemset)\n",
        "        break\n",
        "  ### remove infrequent itemsets from candidate k-itemsets ###\n",
        "  for itemset in infrq_itemsets :\n",
        "    c_k.remove(itemset)\n",
        "\n",
        "  return c_k\n"
      ],
      "metadata": {
        "id": "Ufd1lKSaVv6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# items = ['Bread','Coke', 'Milk','Beer','Diaper','Eggs']\n",
        "# database = [['Bread','Milk'],['Beer','Bread','Diaper','Eggs'],['Beer','Coke','Diaper','Milk'],['Beer','Bread','Diaper','Milk'],['Bread','Coke','Diaper','Milk']]\n",
        "# min_sup_cnt = 3\n",
        "\n",
        "# c_1 = [[item] for item in items] # candidate 1-itemset\n",
        "# c_1 = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "# f_1 = select_freq_itemsets(c_1, min_sup_cnt)\n",
        "\n",
        "# c_2 = generate_candidates(f_1,2) # generate 2-itemset\n",
        "# c_2 = calc_candidate_sup_cnt(database, c_2, 2)\n",
        "# f_2 = select_freq_itemsets(c_2, min_sup_cnt)\n",
        "\n",
        "# c_3 = generate_candidates(f_2,3) # generate 3-itemset\n",
        "# c_3 = calc_candidate_sup_cnt(database, c_3, 3)\n",
        "# f_3 = select_freq_itemsets(c_3, min_sup_cnt)\n",
        "# print(f_1)\n",
        "# print(f_2)\n",
        "# print(f_3)\n",
        "\n",
        "# c_2 = [['Coke', 'Milk'], ['Beer', 'Bread'], ['Bread', 'Diaper'], ['Beer', 'Milk'], ['Diaper', 'Milk'], ['Beer', 'Diaper']]\n",
        "# f_1 = pd.DataFrame({'itemset':[['Bread'],['Beer'],['Diaper']]})\n",
        "# prune_candidates(c_2, f_1, 2)\n",
        "# print(c_2)\n",
        "\n",
        "def apriori_algorithm(min_sup_cnt) :\n",
        "  f_itemsets_store = {}\n",
        "  all_frq_itemsets_df = pd.DataFrame(columns=['item_cnt','itemset','support_cnt'])\n",
        "  for k in itemset_groups:\n",
        "    if(k>1) :\n",
        "      f_prv = f_itemsets_store[k-1]['itemset']\n",
        "      c_k = generate_candidates(f_prv, k) # generate candidates\n",
        "      c_k = prune_candidates(c_k, f_prv, k) # prune candidates\n",
        "      c_k = calc_candidate_sup_cnt(database, c_k, k)\n",
        "      f_k = select_freq_itemsets(c_k, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_k, ignore_index=True)\n",
        "      f_itemsets_store[k] = f_k\n",
        "    else :\n",
        "      c_1 = [[item] for item in unique_items] # candidate 1-itemset\n",
        "      c_1 = calc_candidate_sup_cnt(database, c_1, 1)\n",
        "      f_1 = select_freq_itemsets(c_1, min_sup_cnt)\n",
        "      all_frq_itemsets_df = all_frq_itemsets_df.append(f_1, ignore_index=True)\n",
        "      f_itemsets_store[1] = f_1\n",
        "  return all_frq_itemsets_df, f_itemsets_store\n",
        "\n",
        "# all_itemsets_df = %lprun -f apriori_algorithm apriori_algorithm() # execute line profiler\n",
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2) # execute line profiler\n",
        "print(all_frq_itemsets_df.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GnYQvEz__3Nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98b1a2f-c972-428d-ac9f-d3a60617a9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rule Generation**\n",
        "\n",
        "Antecedent -> Consequent\n",
        "\n",
        "Ex : {X,Y} -> {P,Q}"
      ],
      "metadata": {
        "id": "gDqu5cMCV8LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rules(f_k, h_m, f_itemsets_store, min_conf) :\n",
        "  \"\"\"\n",
        "  Generate rules from frequent itemsets\n",
        "  Inputs:\n",
        "    f_k : Frequent k-itemsets (X,Y,P,Q) : Tuple\n",
        "    h_m : Consequent (P,Q) : Tuple\n",
        "    f_itemsets_store : Frequent itemset store(key = k, value = support counts) : Dictionary(Key = Integer, value = DataFrame)\n",
        "  \"\"\"\n",
        "  k = len(f_k)\n",
        "  m = len(h_m[0])\n",
        "\n",
        "  if(k > m+1) : #### Check whether an item can be passed from antecedent to consequent >>> Terminating condition of the recursive function###\n",
        "    h_next = generate_candidates(h_m, m+1) # Generate (m+1) candidates for consequent\n",
        "    h_next = prune_candidates(h_next, h_m, m+1) # Prune (m+1) candidates\n",
        "    h_next_invalid = []\n",
        "    rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "    for h_next_i in h_next : # Iterate candidate consequents\n",
        "      consequent = h_next_i\n",
        "      antecedent = list(set(f_k) - set(h_next_i))\n",
        "      rule_sup_cnt = f_itemsets_store[k].loc[f_itemsets_store[k]['key']==frozenset(f_k)]\n",
        "      antecedent_sup_cnt = f_itemsets_store[m+1].loc[f_itemsets_store[m+1]['key']==frozenset(h_next_i)]\n",
        "\n",
        "      if len(rule_sup_cnt)>0 and len(antecedent_sup_cnt)>0 :\n",
        "        conf = rule_sup_cnt['support_cnt'].values[0]/antecedent_sup_cnt['support_cnt'].values[0]\n",
        "\n",
        "      if(conf > min_conf) : \n",
        "        data_entry = {\n",
        "          'antecedent': antecedent,\n",
        "          'consequent': consequent,\n",
        "          'confidence': conf\n",
        "        }\n",
        "        rule_df = rule_df.append(data_entry, ignore_index = True)\n",
        "      else : \n",
        "        h_next_invalid.append(h_next_i)\n",
        "\n",
        "    for h_next_invalid_i in h_next_invalid : # Remove invalid consequents \n",
        "      h_next.remove(h_next_invalid_i)\n",
        "\n",
        "    ### recursively generate rules for subgraphs \n",
        "    return generate_rules(f_k, h_next, f_itemsets_store, min_conf).append(rule_df, ignore_index=True)\n",
        "  else :\n",
        "    return pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "  "
      ],
      "metadata": {
        "id": "JglQrjGLWedO"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_frq_itemsets_df, f_itemsets_store = apriori_algorithm(2)\n",
        "min_conf = 0.5\n",
        "\n",
        "rule_df = pd.DataFrame(columns = ['antecedent', 'consequent', 'confidence'])\n",
        "for f_k in all_frq_itemsets_df['itemset'] :\n",
        "  h_m = [[item] for item in f_k]\n",
        "  rule_df = rule_df.append(generate_rules(f_k, h_m, f_itemsets_store, min_conf), ignore_index=True)\n",
        "\n",
        "print(rule_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l5XsaZHP9m0",
        "outputId": "98bdcd95-9002-432b-eee1-afc2ddf2d59d"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      antecedent             consequent  confidence\n",
            "0        [MAGGI]           [BREAD, JAM]    1.000000\n",
            "1          [JAM]         [BREAD, MAGGI]    0.666667\n",
            "2        [BREAD]           [JAM, MAGGI]    1.000000\n",
            "3          [TEA]         [BREAD, MAGGI]    0.666667\n",
            "4          [TEA]       [BISCUIT, MAGGI]    1.000000\n",
            "5        [MAGGI]         [BISCUIT, TEA]    1.000000\n",
            "6   [CORNFLAKES]      [BISCUIT, COFFEE]    1.000000\n",
            "7       [COFFEE]  [BISCUIT, CORNFLAKES]    0.666667\n",
            "8          [TEA]     [BOURNVITA, BREAD]    0.666667\n",
            "9        [BREAD]       [BOURNVITA, TEA]    1.000000\n",
            "10       [BREAD]        [BISCUIT, MILK]    1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FP-Growth Algorithm**"
      ],
      "metadata": {
        "id": "S3kK-iwgTmIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XL8NNI0XTr8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/08/python-tutorial-working-with-csv-file-for-data-science/\n",
        "\n",
        "https://towardsdatascience.com/magic-commands-for-profiling-in-jupyter-notebook-d2ef00e29a63"
      ],
      "metadata": {
        "id": "xhtVLsVvWSJ9"
      }
    }
  ]
}